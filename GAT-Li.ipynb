{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62dade44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ca12bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import ABIDE_Parser as Reader\n",
    "import keras\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl\n",
    "import time\n",
    "import copy\n",
    "import scipy.spatial.distance\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec38117",
   "metadata": {},
   "source": [
    "Some functions in Tensorflow 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49ea0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot(shape, name=None):\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    var = tf.Variable(initial, name=name)\n",
    "    return var\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x,y)\n",
    "    else:\n",
    "        res = tf.matmul(x,y)\n",
    "    return res\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    correct_prediction = tf.equal(tf.round(preds), labels)\n",
    "    accuracy = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy)\n",
    "\n",
    "def tens(shape, name=None):\n",
    "    initial = tf.constant(10, tf.float32, shape)\n",
    "    return tf.Variable(initial, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gat_layer(object):\n",
    "    def __init__(self, input_dim,F_, placeholders,attn_heads=1,attn_heads_reduction='concat',\n",
    "                 activation=tf.nn.relu, use_bias=True,name_=''):\n",
    "        self.dropout_rate = placeholders['dropout']\n",
    "        self.in_drop = placeholders['in_drop']\n",
    "        self.name = 'gat_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = activation\n",
    "        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n",
    "        self.attn_heads_reduction = attn_heads_reduction  #\n",
    "        self.bias = use_bias\n",
    "        self.A = placeholders[\"adj\"]\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            for i in range(self.attn_heads):\n",
    "                self.vars['weights_'+str(i)] = glorot([input_dim, F_], name='weights_' + str(i))\n",
    "                self.vars[\"attn_self_weights_\"+str(i)] = glorot([F_, 1], name='attn_self_weights_' + str(i))\n",
    "                self.vars[\"attn_neighs_weights_\"+str(i)] = glorot([F_, 1], name='attn_neighs_weights_' + str(i))\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([F_],name='bias')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        X = inputs\n",
    "        if self.in_drop != 0.0:\n",
    "            X = tf.nn.dropout(X, 1-self.in_drop)\n",
    "        outputs = []\n",
    "        dense_mask = []\n",
    "\n",
    "        for head in range(self.attn_heads):\n",
    "            # Compute inputs to attention network\n",
    "            kernel = self.vars['weights_'+str(head)]\n",
    "            features = tf.tensordot(X, kernel, axes=1)  # (N x F')\n",
    "\n",
    "            # Compute feature combinations\n",
    "            attention_self_kernel = self.vars[\"attn_self_weights_\"+str(head)]\n",
    "            attention_neighs_kernel = self.vars[\"attn_neighs_weights_\"+str(head)]\n",
    "            attn_for_self = tf.tensordot(features, attention_self_kernel, axes=1)\n",
    "            attn_for_neighs = tf.tensordot(features, attention_neighs_kernel, axes=1)\n",
    "\n",
    "            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
    "            dense = attn_for_self + tf.transpose(attn_for_neighs, [0,2,1])  # (N x N) via broadcasting\n",
    "\n",
    "            #print(\"plus:\", dense.shape)\n",
    "\n",
    "            # Add nonlinearty\n",
    "            dense = tf.nn.leaky_relu(dense,alpha=0.2)\n",
    "\n",
    "            zero_vec = -9e15*tf.ones_like(dense)\n",
    "            dense = tf.where(self.A > 0.0, dense, zero_vec)\n",
    "            dense_mask.append(dense)\n",
    "\n",
    "            # Apply softmax to get attention coefficients\n",
    "            dense = tf.nn.softmax(dense)  # (N x N)\n",
    "\n",
    "            # Apply dropout to features and attention coefficients\n",
    "            dropout_attn = tf.nn.dropout(dense, 1-self.dropout_rate) # (N x N)\n",
    "            dropout_feat = tf.nn.dropout(features, 1-self.dropout_rate)  # (N x F')\n",
    "\n",
    "            # Linear combination with neighbors' features\n",
    "            node_features = tf.matmul(dropout_attn, dropout_feat)  # (N x F')\n",
    "\n",
    "            if self.bias:\n",
    "                node_features += self.vars[\"bias\"]\n",
    "\n",
    "            # Add output of attention head to final output\n",
    "            if self.attn_heads_reduction == 'concat':\n",
    "                outputs.append(self.act(node_features))\n",
    "            else:\n",
    "                outputs.append(node_features)\n",
    "\n",
    "        # Aggregate the heads' output according to the reduction method\n",
    "        if self.attn_heads_reduction == 'concat':\n",
    "            output = tf.concat(outputs, axis=-1)  # (N x KF')\n",
    "        else:\n",
    "            output = tf.add_n(outputs) / self.attn_heads  # N x F')\n",
    "            output = self.act(output)\n",
    "\n",
    "        return output, dense_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eaa51ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,sparse_input=False, act=tf.nn.relu, bias=False, featureless=False,name_=''):\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.name = 'fc_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = act\n",
    "\n",
    "        self.sparse_input = sparse_input\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim], name='weights')\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([output_dim],name='bias')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.nn.dropout(x,1-self.dropout)\n",
    "\n",
    "        output = tf.tensordot(x, self.vars['weights'], axes=1)\n",
    "\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d03e894",
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'node_num' is defined twice. First from /home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/ipykernel_launcher.py, Second from /home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/ipykernel_launcher.py.  Description from first occurrence: Number of Graph nodes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-7f190079f393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'node_num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Number of Graph nodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_dim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Number of output_dim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_integer\u001b[0;34m(name, default, help, lower_bound, upper_bound, flag_values, **args)\u001b[0m\n\u001b[1;32m    313\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegerParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m   \u001b[0m_register_bounds_validator_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'node_num' is defined twice. First from /home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/ipykernel_launcher.py, Second from /home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/ipykernel_launcher.py.  Description from first occurrence: Number of Graph nodes"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer('node_num', 110, 'Number of Graph nodes')\n",
    "\n",
    "flags.DEFINE_integer('output_dim', 1, 'Number of output_dim')\n",
    "flags.DEFINE_float('learning_rate', 0.0001, 'Initial learning rate') #0.0005，0.0001，0.00005，0.00001，0.00003\n",
    "flags.DEFINE_integer('batch_num', 10, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('epochs', 1000, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('attn_heads', 5, 'Number of attention head')\n",
    "\n",
    "flags.DEFINE_integer('hidden1_gat', 24, 'Number of units in hidden layer 1 of gcn')\n",
    "flags.DEFINE_integer('output_gat', 3, 'Number of units in output layer 1 of gcn')\n",
    "\n",
    "flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('in_drop', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 15, 'Tolerance for early stopping (# of epochs).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "883bb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self, placeholders, input_dim):\n",
    "        self.placeholders = placeholders\n",
    "        self.input_dim = input_dim\n",
    "        self.name = 'gat_mil'\n",
    "\n",
    "        self.gat_layers = []\n",
    "        self.fc_layers = []\n",
    "        self.gcn_activations = []\n",
    "        self.fc_activatinos = []\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        self.outputs = None\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "\n",
    "        self.node_prob = None\n",
    "        self.dense_mask = []\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "        self.opt_op = None\n",
    "\n",
    "        self.loss_explainer = 0\n",
    "        #self.optimizer_explainer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01) since im already using tf 1.x so the below code is used\n",
    "        self.optimizer_explainer = tf.train.AdamOptimizer(learning_rate=0.01) #replaced the above line\n",
    "        self.opt_op_explainer = None\n",
    "        self.M = tens((FLAGS.node_num, FLAGS.node_num), name='mask')\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        sigmoid_M = tf.sigmoid(self.M)\n",
    "        self.inputs = tf.multiply(self.inputs, sigmoid_M)\n",
    "\n",
    "        self.gcn_activations.append(self.inputs)\n",
    "\n",
    "        for layer in self.gat_layers:\n",
    "            hidden, dense_mask = layer(self.gcn_activations[-1])\n",
    "            self.gcn_activations.append(hidden)\n",
    "            self.dense_mask.append(dense_mask)\n",
    "\n",
    "\n",
    "        p_layer = self.fc_layers[0]\n",
    "        node_prob = p_layer(self.gcn_activations[-1])\n",
    "\n",
    "        tensor = tf.reshape(node_prob, shape=(-1, FLAGS.node_num))\n",
    "        layer = self.fc_layers[1]\n",
    "        attention_prob = layer(tensor)\n",
    "\n",
    "\n",
    "        attention_mul = tf.multiply(tensor, attention_prob)\n",
    "        self.outputs = tf.reduce_sum(attention_mul, 1, keep_dims=True)\n",
    "        print(self.outputs.shape)\n",
    "\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "\n",
    "        var_list = tf.trainable_variables()\n",
    "        var_list1 = []\n",
    "        for var in var_list:\n",
    "            if var != self.M:\n",
    "                var_list1.append(var)\n",
    "            elif var == self.M:\n",
    "                #stop = input(\"M exit!!!!!!!\")\n",
    "                pass\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss, var_list = [var_list1])\n",
    "        self.loss_explainer += tf.reduce_mean(tf.losses.log_loss(labels=self.placeholders['labels'], predictions=self.outputs))\n",
    "        self.opt_op_explainer = self.optimizer_explainer.minimize(self.loss_explainer, var_list=[self.M])\n",
    "\n",
    "    def _build(self):\n",
    "        self.gat_layers.append(gat_layer(input_dim=self.input_dim,F_=FLAGS.hidden1_gat, placeholders=self.placeholders,\n",
    "                                         attn_heads=FLAGS.attn_heads,attn_heads_reduction='concat',\n",
    "                                         activation=tf.nn.leaky_relu, use_bias=True,name_='1'))\n",
    "\n",
    "        self.gat_layers.append(gat_layer(input_dim=FLAGS.hidden1_gat*FLAGS.attn_heads,F_=FLAGS.output_gat, placeholders=self.placeholders,\n",
    "                                         attn_heads=3,attn_heads_reduction='average',\n",
    "                                         activation=tf.nn.leaky_relu, use_bias=True,name_='2'))\n",
    "\n",
    "        self.fc_layers.append(fc_layer(input_dim=FLAGS.output_gat, output_dim=FLAGS.output_dim, placeholders=self.placeholders,\n",
    "                                       act=tf.nn.sigmoid, dropout=True, name_='1'))\n",
    "\n",
    "        self.fc_layers.append(fc_layer(input_dim=FLAGS.node_num, output_dim=FLAGS.node_num, placeholders=self.placeholders,\n",
    "                                       act=tf.nn.softmax, dropout=True, name_='2'))\n",
    "\n",
    "    def _loss(self):\n",
    "        for var in self.gat_layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay*tf.nn.l2_loss(var)\n",
    "\n",
    "\n",
    "        self.loss += tf.reduce_mean(tf.losses.log_loss(labels=self.placeholders['labels'], predictions=self.outputs))\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = accuracy(self.outputs, self.placeholders['labels'])\n",
    "\n",
    "    def predict(self):\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41d1f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '/home/celery/Documents/Research/dataset/Outputs/'\n",
    "data_folder = os.path.join(root_folder, 'cpac/filt_noglobal')\n",
    "subject_IDs = np.genfromtxt('/home/celery/Documents/Research/AI_Tsinghua_Au_2025/Code/IDs/under15_short_IDs.txt', dtype=str)\n",
    "subject_IDs = subject_IDs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d92db3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get label\n",
    "label_dict = Reader.get_label(subject_IDs)\n",
    "label_list = np.array([int(label_dict[x]) for x in subject_IDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13ae19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/celery/Documents/Research/dataset/Outputs/cpac/filt_global/mat/'\n",
    "def load_connectivity(subject_list, kind, atlas_name = 'ho'):\n",
    "    all_networks = []\n",
    "    for subject in subject_list:\n",
    "        fl = os.path.join(data_folder,\n",
    "                          subject + \"_\" + atlas_name + \"_\" + kind + \".mat\")\n",
    "        matrix = sio.loadmat(fl)['connectivity']\n",
    "        if atlas_name == 'ho':\n",
    "            matrix = np.delete(matrix, 82, axis=0)\n",
    "            matrix = np.delete(matrix, 82, axis=1)\n",
    "        all_networks.append(matrix)\n",
    "    all_networks=np.array(all_networks)\n",
    "    return all_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0339601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getconn_vector(subject_name0, kind, atlas):\n",
    "    subject_name = np.array(subject_name0)\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    conn_array = load_connectivity(subject_name, kind, atlas)\n",
    "    data_x = np.array(conn_array)\n",
    "\n",
    "    for subname in subject_name:\n",
    "        data_y.append([int(label_dict[subname])])\n",
    "\n",
    "    data_y = np.array(data_y)\n",
    "    return data_x, data_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "edeaf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = getconn_vector(subject_IDs, \"correlation\", \"ho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26ecd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_x = map(abs, X)\n",
    "adjs = np.array(list(abs_x))\n",
    "features = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "632c093e",
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'f' is defined twice. First from /home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/ipykernel_launcher.py, Second from /home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/ipykernel_launcher.py.  Description from first occurrence: kernel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-992305738b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kernel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_string\u001b[0;34m(name, default, help, flag_values, **args)\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'f' is defined twice. First from /home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/ipykernel_launcher.py, Second from /home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/ipykernel_launcher.py.  Description from first occurrence: kernel"
     ]
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b014108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'adj': tf.placeholder(tf.float32, shape=(None,FLAGS.node_num,FLAGS.node_num)),\n",
    "    'features': tf.placeholder(tf.float32, shape=(None,FLAGS.node_num,FLAGS.node_num)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, 1)),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'in_drop': tf.placeholder_with_default(0., shape=()),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5426dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feed_dict(features, support, labels, placeholders):\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['adj']: support})\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91df014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "217cf587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(adjs, features, y):\n",
    "    shuffle_ix = np.random.permutation(np.arange(len(y)))\n",
    "    adjs = adjs[shuffle_ix]\n",
    "    features = features[shuffle_ix]\n",
    "    y = y[shuffle_ix]\n",
    "    return adjs, features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af1ff071",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_index = []\n",
    "all_train_index = []\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(subject_IDs, label_list):\n",
    "    all_train_index.append(train_index)\n",
    "    all_test_index.append(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33243a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11bec599",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "l1, l2 = [], []\n",
    "\n",
    "k = 0 #fold number\n",
    "train_index = all_train_index[k]\n",
    "test_index = all_test_index[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f57e21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModelSavePath0 = '/home/celery/Documents/Research/AI_Tsinghua_Au_2025/Code/fold_e_mask%s/gat_e%s_weights_best.ckpt' % (str(k),str(k))\n",
    "bestModelSavePath1 = '/home/celery/Documents/Research/AI_Tsinghua_Au_2025/Code/fold_e_mask%s/gat_e%s_weights_best.ckpt' % (str(k),str(k))\n",
    "features_train, features_test = features[train_index], features[test_index]\n",
    "support_train, support_test = adjs[train_index], adjs[test_index]\n",
    "y_train, y_test = Y[train_index], Y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2a9b2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test shape: (92, 110, 110) support shape: (365, 110, 110)\n",
      "y_test [[0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "support_train, features_train, y_train = shuffle(support_train, features_train, y_train)\n",
    "support_test, features_test, y_test = shuffle(support_test, features_test, y_test)\n",
    "print(\"test shape:\",features_test.shape, \"support shape:\", support_train.shape)\n",
    "print(\"y_test\", y_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9e6daea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "model = Model(placeholders, input_dim=features.shape[2])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "'''设置模型保存器'''\n",
    "m_saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ab78972",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_val = []\n",
    "batch_num = FLAGS.batch_num\n",
    "train_num = features_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7a7724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.74487 train_acc= 0.54722 test_loss= 0.74499 test_acc= 0.59783 time= 3.35104\n",
      "Epoch: 0002 train_loss= 0.74242 train_acc= 0.55556 test_loss= 0.74290 test_acc= 0.59692 time= 2.36920\n",
      "Epoch: 0003 train_loss= 0.74028 train_acc= 0.54722 test_loss= 0.74134 test_acc= 0.56703 time= 2.34504\n",
      "Epoch: 0004 train_loss= 0.73829 train_acc= 0.55278 test_loss= 0.73980 test_acc= 0.56522 time= 2.33247\n",
      "Epoch: 0005 train_loss= 0.73632 train_acc= 0.55556 test_loss= 0.73841 test_acc= 0.56522 time= 2.32208\n",
      "Epoch: 0006 train_loss= 0.73438 train_acc= 0.55833 test_loss= 0.73704 test_acc= 0.56401 time= 2.37507\n",
      "Epoch: 0007 train_loss= 0.73251 train_acc= 0.55556 test_loss= 0.73571 test_acc= 0.55435 time= 2.37493\n",
      "Epoch: 0008 train_loss= 0.73067 train_acc= 0.55278 test_loss= 0.73439 test_acc= 0.55435 time= 2.35978\n",
      "Epoch: 0009 train_loss= 0.72881 train_acc= 0.55556 test_loss= 0.73305 test_acc= 0.55556 time= 2.36609\n",
      "Epoch: 0010 train_loss= 0.72693 train_acc= 0.55556 test_loss= 0.73171 test_acc= 0.56220 time= 2.41556\n",
      "Epoch: 0011 train_loss= 0.72503 train_acc= 0.55833 test_loss= 0.73042 test_acc= 0.56522 time= 2.38958\n",
      "Epoch: 0012 train_loss= 0.72312 train_acc= 0.56389 test_loss= 0.72917 test_acc= 0.56522 time= 2.42313\n",
      "Epoch: 0013 train_loss= 0.72120 train_acc= 0.56389 test_loss= 0.72791 test_acc= 0.56522 time= 2.41163\n",
      "Epoch: 0014 train_loss= 0.71925 train_acc= 0.56389 test_loss= 0.72664 test_acc= 0.56522 time= 2.45539\n",
      "Epoch: 0015 train_loss= 0.71725 train_acc= 0.56944 test_loss= 0.72531 test_acc= 0.56522 time= 2.47003\n",
      "Epoch: 0016 train_loss= 0.71521 train_acc= 0.58056 test_loss= 0.72396 test_acc= 0.56522 time= 2.46550\n",
      "Epoch: 0017 train_loss= 0.71310 train_acc= 0.58611 test_loss= 0.72259 test_acc= 0.56522 time= 2.52117\n",
      "Epoch: 0018 train_loss= 0.71096 train_acc= 0.58889 test_loss= 0.72121 test_acc= 0.55857 time= 2.52674\n",
      "Epoch: 0019 train_loss= 0.70876 train_acc= 0.59167 test_loss= 0.71980 test_acc= 0.55495 time= 2.51081\n",
      "Epoch: 0020 train_loss= 0.70654 train_acc= 0.59167 test_loss= 0.71842 test_acc= 0.56492 time= 2.51645\n",
      "Epoch: 0021 train_loss= 0.70424 train_acc= 0.59722 test_loss= 0.71703 test_acc= 0.57971 time= 2.54430\n",
      "Epoch: 0022 train_loss= 0.70202 train_acc= 0.59722 test_loss= 0.71565 test_acc= 0.58696 time= 2.56442\n",
      "Epoch: 0023 train_loss= 0.69980 train_acc= 0.60278 test_loss= 0.71435 test_acc= 0.58545 time= 2.56862\n",
      "Epoch: 0024 train_loss= 0.69753 train_acc= 0.60556 test_loss= 0.71308 test_acc= 0.58243 time= 2.56639\n",
      "Epoch: 0025 train_loss= 0.69521 train_acc= 0.61389 test_loss= 0.71180 test_acc= 0.58696 time= 2.58310\n",
      "Epoch: 0026 train_loss= 0.69285 train_acc= 0.61389 test_loss= 0.71044 test_acc= 0.59028 time= 2.60788\n",
      "Epoch: 0027 train_loss= 0.69040 train_acc= 0.62222 test_loss= 0.70901 test_acc= 0.58907 time= 2.61718\n",
      "Epoch: 0028 train_loss= 0.68786 train_acc= 0.61944 test_loss= 0.70763 test_acc= 0.58545 time= 2.64695\n",
      "Epoch: 0029 train_loss= 0.68536 train_acc= 0.62778 test_loss= 0.70630 test_acc= 0.58333 time= 2.62590\n",
      "Epoch: 0030 train_loss= 0.68283 train_acc= 0.63611 test_loss= 0.70501 test_acc= 0.58243 time= 2.67238\n",
      "Epoch: 0031 train_loss= 0.68026 train_acc= 0.64167 test_loss= 0.70372 test_acc= 0.57911 time= 2.73531\n",
      "Epoch: 0032 train_loss= 0.67769 train_acc= 0.65000 test_loss= 0.70247 test_acc= 0.58031 time= 2.73035\n",
      "Epoch: 0033 train_loss= 0.67513 train_acc= 0.65833 test_loss= 0.70126 test_acc= 0.58575 time= 2.72250\n",
      "Epoch: 0034 train_loss= 0.67255 train_acc= 0.66389 test_loss= 0.70007 test_acc= 0.59149 time= 2.67323\n",
      "Epoch: 0035 train_loss= 0.66993 train_acc= 0.66944 test_loss= 0.69889 test_acc= 0.59873 time= 2.65857\n",
      "Epoch: 0036 train_loss= 0.66726 train_acc= 0.68056 test_loss= 0.69775 test_acc= 0.60749 time= 2.71535\n",
      "Epoch: 0037 train_loss= 0.66455 train_acc= 0.68333 test_loss= 0.69655 test_acc= 0.61715 time= 2.75729\n",
      "Epoch: 0038 train_loss= 0.66178 train_acc= 0.68333 test_loss= 0.69534 test_acc= 0.62711 time= 2.84755\n",
      "Epoch: 0039 train_loss= 0.65896 train_acc= 0.68056 test_loss= 0.69418 test_acc= 0.63376 time= 2.70438\n",
      "Epoch: 0040 train_loss= 0.65614 train_acc= 0.68056 test_loss= 0.69313 test_acc= 0.63436 time= 2.77816\n",
      "Epoch: 0041 train_loss= 0.65337 train_acc= 0.68056 test_loss= 0.69202 test_acc= 0.63315 time= 2.73437\n",
      "Epoch: 0042 train_loss= 0.65053 train_acc= 0.68333 test_loss= 0.69083 test_acc= 0.63496 time= 2.79142\n",
      "Epoch: 0043 train_loss= 0.64776 train_acc= 0.69444 test_loss= 0.68966 test_acc= 0.62893 time= 2.84246\n",
      "Epoch: 0044 train_loss= 0.64496 train_acc= 0.69722 test_loss= 0.68868 test_acc= 0.62742 time= 2.82713\n",
      "Epoch: 0045 train_loss= 0.64213 train_acc= 0.70000 test_loss= 0.68760 test_acc= 0.62409 time= 2.75466\n",
      "Epoch: 0046 train_loss= 0.63921 train_acc= 0.70556 test_loss= 0.68658 test_acc= 0.61957 time= 2.83070\n",
      "Epoch: 0047 train_loss= 0.63634 train_acc= 0.70833 test_loss= 0.68561 test_acc= 0.62047 time= 2.80297\n",
      "Epoch: 0048 train_loss= 0.63348 train_acc= 0.71389 test_loss= 0.68461 test_acc= 0.62258 time= 2.83666\n",
      "Epoch: 0049 train_loss= 0.63066 train_acc= 0.71667 test_loss= 0.68376 test_acc= 0.62319 time= 2.90733\n",
      "Epoch: 0050 train_loss= 0.62776 train_acc= 0.71944 test_loss= 0.68272 test_acc= 0.62077 time= 2.83676\n",
      "Epoch: 0051 train_loss= 0.62489 train_acc= 0.71667 test_loss= 0.68173 test_acc= 0.62198 time= 2.82911\n",
      "Epoch: 0052 train_loss= 0.62200 train_acc= 0.72222 test_loss= 0.68075 test_acc= 0.61836 time= 2.83446\n",
      "Epoch: 0053 train_loss= 0.61909 train_acc= 0.72778 test_loss= 0.67983 test_acc= 0.61685 time= 2.77149\n",
      "Epoch: 0054 train_loss= 0.61616 train_acc= 0.73333 test_loss= 0.67904 test_acc= 0.61655 time= 2.76300\n",
      "Epoch: 0055 train_loss= 0.61320 train_acc= 0.74167 test_loss= 0.67824 test_acc= 0.61957 time= 2.86112\n",
      "Epoch: 0056 train_loss= 0.61030 train_acc= 0.74722 test_loss= 0.67753 test_acc= 0.62319 time= 2.87609\n",
      "Epoch: 0057 train_loss= 0.60734 train_acc= 0.76111 test_loss= 0.67670 test_acc= 0.63255 time= 2.81976\n",
      "Epoch: 0058 train_loss= 0.60446 train_acc= 0.76389 test_loss= 0.67592 test_acc= 0.63768 time= 2.79537\n",
      "Epoch: 0059 train_loss= 0.60154 train_acc= 0.76667 test_loss= 0.67524 test_acc= 0.64070 time= 2.90821\n",
      "Epoch: 0060 train_loss= 0.59859 train_acc= 0.76944 test_loss= 0.67460 test_acc= 0.64342 time= 2.86906\n",
      "Epoch: 0061 train_loss= 0.59562 train_acc= 0.77222 test_loss= 0.67393 test_acc= 0.64704 time= 2.85222\n",
      "Epoch: 0062 train_loss= 0.59271 train_acc= 0.77778 test_loss= 0.67324 test_acc= 0.64825 time= 2.84779\n",
      "Epoch: 0063 train_loss= 0.58979 train_acc= 0.78056 test_loss= 0.67261 test_acc= 0.65278 time= 2.89087\n",
      "Epoch: 0064 train_loss= 0.58687 train_acc= 0.78333 test_loss= 0.67197 test_acc= 0.65580 time= 2.85280\n",
      "Epoch: 0065 train_loss= 0.58401 train_acc= 0.78056 test_loss= 0.67144 test_acc= 0.65731 time= 2.86255\n",
      "Epoch: 0066 train_loss= 0.58113 train_acc= 0.78333 test_loss= 0.67086 test_acc= 0.66214 time= 2.86441\n",
      "Epoch: 0067 train_loss= 0.57827 train_acc= 0.78889 test_loss= 0.67031 test_acc= 0.66667 time= 2.87994\n",
      "Epoch: 0068 train_loss= 0.57541 train_acc= 0.78611 test_loss= 0.66979 test_acc= 0.66848 time= 2.92482\n",
      "Epoch: 0069 train_loss= 0.57256 train_acc= 0.78611 test_loss= 0.66929 test_acc= 0.66878 time= 2.91849\n",
      "Epoch: 0070 train_loss= 0.56972 train_acc= 0.78889 test_loss= 0.66889 test_acc= 0.66727 time= 2.88072\n",
      "Epoch: 0071 train_loss= 0.56685 train_acc= 0.79167 test_loss= 0.66840 test_acc= 0.66606 time= 2.93330\n",
      "Epoch: 0072 train_loss= 0.56400 train_acc= 0.79167 test_loss= 0.66800 test_acc= 0.66516 time= 2.84543\n",
      "Epoch: 0073 train_loss= 0.56115 train_acc= 0.79722 test_loss= 0.66765 test_acc= 0.66455 time= 2.92499\n",
      "Epoch: 0074 train_loss= 0.55831 train_acc= 0.80556 test_loss= 0.66718 test_acc= 0.66636 time= 2.88703\n",
      "Epoch: 0075 train_loss= 0.55549 train_acc= 0.80556 test_loss= 0.66678 test_acc= 0.66878 time= 2.88106\n",
      "Epoch: 0076 train_loss= 0.55266 train_acc= 0.80278 test_loss= 0.66638 test_acc= 0.66969 time= 2.91756\n",
      "Epoch: 0077 train_loss= 0.54985 train_acc= 0.80278 test_loss= 0.66602 test_acc= 0.66878 time= 2.84196\n",
      "Epoch: 0078 train_loss= 0.54704 train_acc= 0.80278 test_loss= 0.66560 test_acc= 0.67210 time= 2.91456\n",
      "Epoch: 0079 train_loss= 0.54424 train_acc= 0.80556 test_loss= 0.66527 test_acc= 0.67089 time= 2.91654\n",
      "Epoch: 0080 train_loss= 0.54144 train_acc= 0.80833 test_loss= 0.66493 test_acc= 0.67301 time= 2.86559\n",
      "Epoch: 0081 train_loss= 0.53869 train_acc= 0.81111 test_loss= 0.66467 test_acc= 0.67210 time= 2.88437\n",
      "Epoch: 0082 train_loss= 0.53592 train_acc= 0.81667 test_loss= 0.66439 test_acc= 0.67301 time= 2.95046\n",
      "Epoch: 0083 train_loss= 0.53317 train_acc= 0.81944 test_loss= 0.66414 test_acc= 0.67482 time= 2.95267\n",
      "Epoch: 0084 train_loss= 0.53047 train_acc= 0.81667 test_loss= 0.66398 test_acc= 0.67301 time= 2.95320\n",
      "Epoch: 0085 train_loss= 0.52776 train_acc= 0.81667 test_loss= 0.66376 test_acc= 0.67331 time= 2.88504\n",
      "Epoch: 0086 train_loss= 0.52507 train_acc= 0.81667 test_loss= 0.66361 test_acc= 0.67180 time= 3.01031\n",
      "Epoch: 0087 train_loss= 0.52234 train_acc= 0.81944 test_loss= 0.66347 test_acc= 0.67361 time= 2.92909\n",
      "Epoch: 0088 train_loss= 0.51964 train_acc= 0.82222 test_loss= 0.66332 test_acc= 0.67572 time= 2.86162\n",
      "Epoch: 0089 train_loss= 0.51693 train_acc= 0.82500 test_loss= 0.66326 test_acc= 0.67754 time= 2.90035\n",
      "Epoch: 0090 train_loss= 0.51425 train_acc= 0.82500 test_loss= 0.66321 test_acc= 0.67874 time= 2.86554\n",
      "Epoch: 0091 train_loss= 0.51156 train_acc= 0.82500 test_loss= 0.66318 test_acc= 0.67935 time= 2.88237\n",
      "Epoch: 0092 train_loss= 0.50889 train_acc= 0.83333 test_loss= 0.66313 test_acc= 0.68297 time= 2.88104\n",
      "Epoch: 0093 train_loss= 0.50623 train_acc= 0.83889 test_loss= 0.66308 test_acc= 0.68629 time= 2.85239\n",
      "Epoch: 0094 train_loss= 0.50359 train_acc= 0.83889 test_loss= 0.66314 test_acc= 0.68780 time= 2.90200\n",
      "Epoch: 0095 train_loss= 0.50095 train_acc= 0.83889 test_loss= 0.66313 test_acc= 0.69022 time= 2.86646\n",
      "Epoch: 0096 train_loss= 0.49832 train_acc= 0.83889 test_loss= 0.66318 test_acc= 0.69112 time= 2.90305\n",
      "Epoch: 0097 train_loss= 0.49570 train_acc= 0.84444 test_loss= 0.66319 test_acc= 0.69263 time= 2.95293\n",
      "Epoch: 0098 train_loss= 0.49308 train_acc= 0.84444 test_loss= 0.66325 test_acc= 0.69263 time= 2.90051\n",
      "Epoch: 0099 train_loss= 0.49049 train_acc= 0.84722 test_loss= 0.66340 test_acc= 0.69293 time= 2.90470\n",
      "early_stopping...\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(FLAGS.epochs):  #FLAGS.epochs\n",
    "    batches = range(round(train_num / batch_num))\n",
    "\n",
    "    costs = np.zeros((len(batches), 2))\n",
    "    accs = np.zeros((len(batches), 2))\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    for ib in batches:\n",
    "        from_i = ib * batch_num\n",
    "        to_i = (ib+1) * batch_num\n",
    "\n",
    "        features_train_batch = features_train[from_i:to_i]\n",
    "        support_train_batch = support_train[from_i:to_i]\n",
    "        y_train_batch = y_train[from_i:to_i]\n",
    "        feed_dict = construct_feed_dict(features_train_batch, support_train_batch, y_train_batch, placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        feed_dict.update({placeholders['in_drop']: FLAGS.in_drop})\n",
    "\n",
    "        outs = sess.run([model.opt_op, model.loss, model.accuracy],feed_dict=feed_dict)\n",
    "\n",
    "        cost,acc,duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "        costs[ib] = [outs[1], cost]\n",
    "        accs[ib] = [outs[2], acc]\n",
    "\n",
    "    costs = costs.mean(axis=0)\n",
    "    cost_train, cost_test = costs\n",
    "    cost_val.append(cost_test)\n",
    "\n",
    "    accs = accs.mean(axis=0)\n",
    "    acc_train, acc_test = accs\n",
    "\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cost_train),\n",
    "        \"train_acc=\", \"{:.5f}\".format(acc_train), \"test_loss=\", \"{:.5f}\".format(cost_test),\n",
    "        \"test_acc=\", \"{:.5f}\".format(acc_test), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"early_stopping...\")\n",
    "        break\n",
    "\n",
    "m_saver.save(sess, bestModelSavePath0)\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "261caa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cost, test_acc, test_duration = evaluate(features_test, support_test, y_test, placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ef32695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.66422 accuracy= 0.68478 time= 0.057363\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "    \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.6f}\".format(test_duration))\n",
    "l1.append(test_acc)\n",
    "test_pre = model.predict()\n",
    "feed_dict = construct_feed_dict(features_test, support_test, y_test, placeholders)\n",
    "test_pre = sess.run([test_pre],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68d9c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for p in test_pre[0]:\n",
    "    y_pred.append(round(p[0]))\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0e674d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, sensivity, specificity, fscore, auc: 0.6847826086956522 0.5853658536585366 0.7647058823529411 0.6233766233766234 0.7326637972262076\n"
     ]
    }
   ],
   "source": [
    "[[TN, FP], [FN, TP]] = confusion_matrix(y_test, y_pred, labels=[0, 1]).astype(float)\n",
    "acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "specificity = TN/(FP+TN)\n",
    "sensitivity = recall = TP/(TP+FN)\n",
    "fscore = f1_score(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,test_pre[0])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"accuracy, sensivity, specificity, fscore, auc:\", acc, sensitivity, specificity, fscore, roc_auc)\n",
    "result = [acc, sensitivity, specificity, fscore, roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea0c7a38",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1cf6d6fdbb7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_op_explainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_explainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mcosts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mib\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0maccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mib\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-2c0d3c20cc97>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(features, support, labels, placeholders)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mt_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfeed_dict_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mouts_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mouts_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m           \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m           \u001b[0mfeed_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/GAT/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operation was not named: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s:%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cost_val = []\n",
    "for epoch in range(400):  #FLAGS.epochs\n",
    "    batches = range(round(train_num / batch_num))\n",
    "\n",
    "    costs = np.zeros((len(batches), 2))\n",
    "    accs = np.zeros((len(batches), 2))\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    for ib in batches:\n",
    "        from_i = ib * batch_num\n",
    "        to_i = (ib+1) * batch_num\n",
    "\n",
    "        features_train_batch = features_train[from_i:to_i]\n",
    "        support_train_batch = support_train[from_i:to_i]\n",
    "        y_train_batch = y_train[from_i:to_i]\n",
    "        feed_dict = construct_feed_dict(features_train_batch, support_train_batch, y_train_batch, placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        feed_dict.update({placeholders['in_drop']: FLAGS.in_drop})\n",
    "\n",
    "        outs = sess.run([model.opt_op_explainer, model.loss_explainer, model.accuracy],feed_dict=feed_dict)\n",
    "\n",
    "        cost,acc,duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "        costs[ib] = [outs[1], cost]\n",
    "        accs[ib] = [outs[2], acc]\n",
    "\n",
    "    costs = costs.mean(axis=0)\n",
    "    cost_train, cost_test = costs\n",
    "    cost_val.append(cost_test)\n",
    "\n",
    "    accs = accs.mean(axis=0)\n",
    "    acc_train, acc_test = accs\n",
    "\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cost_train),\n",
    "        \"train_acc=\", \"{:.5f}\".format(acc_train), \"test_loss=\", \"{:.5f}\".format(cost_test),\n",
    "        \"test_acc=\", \"{:.5f}\".format(acc_test), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "m_saver.save(sess, bestModelSavePath1)\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b542681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.65684 accuracy= 0.67797 time= 0.11780\n"
     ]
    }
   ],
   "source": [
    "test_cost, test_acc, test_duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "    \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "l2.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6723164]\n",
      "[0.6779661]\n",
      "[0.6610169491525424, 0.5802469135802469, 0.7291666666666666, 0.6103896103896104, 0.715406378600823]\n"
     ]
    }
   ],
   "source": [
    "M = model.M\n",
    "s_m = tf.sigmoid(M)\n",
    "M = M.eval(session=sess)\n",
    "s_m = s_m.eval(session=sess)\n",
    "\n",
    "# Custom save directory\n",
    "save_dir = \"/home/celery/Documents/Research/AI_Tsinghua_Au_2025/Code/weights/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Format file path using Python 3.5 compatible style\n",
    "save_path = os.path.join(save_dir, \"fold{}_mask.pkl\".format(k))\n",
    "\n",
    "# Save the sigmoid mask\n",
    "with open(save_path, 'wb+') as f:\n",
    "    pkl.dump(s_m, f)\n",
    "\n",
    "sess.close()\n",
    "\n",
    "\n",
    "print(l1)\n",
    "print(l2)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
