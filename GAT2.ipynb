{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d811ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047cb0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import ABIDE_Parser as Reader\n",
    "import keras\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl\n",
    "import time\n",
    "import copy\n",
    "import scipy.spatial.distance\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b66ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot(shape, name=None):\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    var = tf.Variable(initial, name=name)\n",
    "    return var\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x,y)\n",
    "    else:\n",
    "        res = tf.matmul(x,y)\n",
    "    return res\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    correct_prediction = tf.equal(tf.round(preds), labels)\n",
    "    accuracy = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy)\n",
    "\n",
    "def tens(shape, name=None):\n",
    "    initial = tf.constant(10, tf.float32, shape)\n",
    "    return tf.Variable(initial, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba3721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gat_layer(object):\n",
    "    def __init__(self, input_dim,F_, placeholders,attn_heads=1,attn_heads_reduction='concat',\n",
    "                 activation=tf.nn.relu, use_bias=True,name_=''):\n",
    "        self.dropout_rate = placeholders['dropout']\n",
    "        self.in_drop = placeholders['in_drop']\n",
    "        self.name = 'gat_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = activation\n",
    "        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n",
    "        self.attn_heads_reduction = attn_heads_reduction  #\n",
    "        self.bias = use_bias\n",
    "        self.A = placeholders[\"adj\"]\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            for i in range(self.attn_heads):\n",
    "                self.vars['weights_'+str(i)] = glorot([input_dim, F_], name='weights_' + str(i))\n",
    "                self.vars[\"attn_self_weights_\"+str(i)] = glorot([F_, 1], name='attn_self_weights_' + str(i))\n",
    "                self.vars[\"attn_neighs_weights_\"+str(i)] = glorot([F_, 1], name='attn_neighs_weights_' + str(i))\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([F_],name='bias')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        X = inputs\n",
    "        if self.in_drop != 0.0:\n",
    "            X = tf.nn.dropout(X, 1-self.in_drop)\n",
    "        outputs = []\n",
    "        dense_mask = []\n",
    "\n",
    "        for head in range(self.attn_heads):\n",
    "            # Compute inputs to attention network\n",
    "            kernel = self.vars['weights_'+str(head)]\n",
    "            features = tf.tensordot(X, kernel, axes=1)  # (N x F')\n",
    "\n",
    "            # Compute feature combinations\n",
    "            attention_self_kernel = self.vars[\"attn_self_weights_\"+str(head)]\n",
    "            attention_neighs_kernel = self.vars[\"attn_neighs_weights_\"+str(head)]\n",
    "            attn_for_self = tf.tensordot(features, attention_self_kernel, axes=1)\n",
    "            attn_for_neighs = tf.tensordot(features, attention_neighs_kernel, axes=1)\n",
    "\n",
    "            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
    "            dense = attn_for_self + tf.transpose(attn_for_neighs, [0,2,1])  # (N x N) via broadcasting\n",
    "\n",
    "            #print(\"plus:\", dense.shape)\n",
    "\n",
    "            # Add nonlinearty\n",
    "            dense = tf.nn.leaky_relu(dense,alpha=0.2)\n",
    "\n",
    "            zero_vec = -9e15*tf.ones_like(dense)\n",
    "            dense = tf.where(self.A > 0.0, dense, zero_vec)\n",
    "            dense_mask.append(dense)\n",
    "\n",
    "            # Apply softmax to get attention coefficients\n",
    "            dense = tf.nn.softmax(dense)  # (N x N)\n",
    "\n",
    "            # Apply dropout to features and attention coefficients\n",
    "            dropout_attn = tf.nn.dropout(dense, 1-self.dropout_rate) # (N x N)\n",
    "            dropout_feat = tf.nn.dropout(features, 1-self.dropout_rate)  # (N x F')\n",
    "\n",
    "            # Linear combination with neighbors' features\n",
    "            node_features = tf.matmul(dropout_attn, dropout_feat)  # (N x F')\n",
    "\n",
    "            if self.bias:\n",
    "                node_features += self.vars[\"bias\"]\n",
    "\n",
    "            # Add output of attention head to final output\n",
    "            if self.attn_heads_reduction == 'concat':\n",
    "                outputs.append(self.act(node_features))\n",
    "            else:\n",
    "                outputs.append(node_features)\n",
    "\n",
    "        # Aggregate the heads' output according to the reduction method\n",
    "        if self.attn_heads_reduction == 'concat':\n",
    "            output = tf.concat(outputs, axis=-1)  # (N x KF')\n",
    "        else:\n",
    "            output = tf.add_n(outputs) / self.attn_heads  # N x F')\n",
    "            output = self.act(output)\n",
    "\n",
    "        return output, dense_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294b7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,sparse_input=False, act=tf.nn.relu, bias=False, featureless=False,name_=''):\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.name = 'fc_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = act\n",
    "\n",
    "        self.sparse_input = sparse_input\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim], name='weights')\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([output_dim],name='bias')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.nn.dropout(x,1-self.dropout)\n",
    "\n",
    "        output = tf.tensordot(x, self.vars['weights'], axes=1)\n",
    "\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5db532d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer('node_num', 110, 'Number of Graph nodes')\n",
    "\n",
    "flags.DEFINE_integer('output_dim', 1, 'Number of output_dim')\n",
    "flags.DEFINE_float('learning_rate', 0.0001, 'Initial learning rate') #0.0005，0.0001，0.00005，0.00001，0.00003\n",
    "flags.DEFINE_integer('batch_num', 10, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('epochs', 1000, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('attn_heads', 5, 'Number of attention head')\n",
    "\n",
    "flags.DEFINE_integer('hidden1_gat', 24, 'Number of units in hidden layer 1 of gcn')\n",
    "flags.DEFINE_integer('output_gat', 3, 'Number of units in output layer 1 of gcn')\n",
    "\n",
    "flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('in_drop', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 15, 'Tolerance for early stopping (# of epochs).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f690a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self, placeholders, input_dim):\n",
    "        self.placeholders = placeholders\n",
    "        self.input_dim = input_dim\n",
    "        self.name = 'gat_mil'\n",
    "\n",
    "        self.gat_layers = []\n",
    "        self.fc_layers = []\n",
    "        self.gcn_activations = []\n",
    "        self.fc_activatinos = []\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        self.outputs = None\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "\n",
    "        self.node_prob = None\n",
    "        self.dense_mask = []\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "        self.opt_op = None\n",
    "\n",
    "        self.loss_explainer = 0\n",
    "        #self.optimizer_explainer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01) since im already using tf 1.x so the below code is used\n",
    "        self.optimizer_explainer = tf.train.AdamOptimizer(learning_rate=0.01) #replaced the above line\n",
    "        self.opt_op_explainer = None\n",
    "        self.M = tens((FLAGS.node_num, FLAGS.node_num), name='mask')\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        sigmoid_M = tf.sigmoid(self.M)\n",
    "        self.inputs = tf.multiply(self.inputs, sigmoid_M)\n",
    "\n",
    "        self.gcn_activations.append(self.inputs)\n",
    "\n",
    "        for layer in self.gat_layers:\n",
    "            hidden, dense_mask = layer(self.gcn_activations[-1])\n",
    "            self.gcn_activations.append(hidden)\n",
    "            self.dense_mask.append(dense_mask)\n",
    "\n",
    "\n",
    "        p_layer = self.fc_layers[0]\n",
    "        node_prob = p_layer(self.gcn_activations[-1])\n",
    "\n",
    "        tensor = tf.reshape(node_prob, shape=(-1, FLAGS.node_num))\n",
    "        layer = self.fc_layers[1]\n",
    "        attention_prob = layer(tensor)\n",
    "\n",
    "\n",
    "        attention_mul = tf.multiply(tensor, attention_prob)\n",
    "        self.outputs = tf.reduce_sum(attention_mul, 1, keep_dims=True)\n",
    "        print(self.outputs.shape)\n",
    "\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "\n",
    "        var_list = tf.trainable_variables()\n",
    "        var_list1 = []\n",
    "        for var in var_list:\n",
    "            if var != self.M:\n",
    "                var_list1.append(var)\n",
    "            elif var == self.M:\n",
    "                #stop = input(\"M exit!!!!!!!\")\n",
    "                pass\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss, var_list = [var_list1])\n",
    "        self.loss_explainer += tf.reduce_mean(tf.losses.log_loss(labels=self.placeholders['labels'], predictions=self.outputs))\n",
    "        self.opt_op_explainer = self.optimizer_explainer.minimize(self.loss_explainer, var_list=[self.M])\n",
    "\n",
    "    def _build(self):\n",
    "        self.gat_layers.append(gat_layer(input_dim=self.input_dim,F_=FLAGS.hidden1_gat, placeholders=self.placeholders,\n",
    "                                         attn_heads=FLAGS.attn_heads,attn_heads_reduction='concat',\n",
    "                                         activation=tf.nn.leaky_relu, use_bias=True,name_='1'))\n",
    "\n",
    "        self.gat_layers.append(gat_layer(input_dim=FLAGS.hidden1_gat*FLAGS.attn_heads,F_=FLAGS.output_gat, placeholders=self.placeholders,\n",
    "                                         attn_heads=3,attn_heads_reduction='average',\n",
    "                                         activation=tf.nn.leaky_relu, use_bias=True,name_='2'))\n",
    "\n",
    "        self.fc_layers.append(fc_layer(input_dim=FLAGS.output_gat, output_dim=FLAGS.output_dim, placeholders=self.placeholders,\n",
    "                                       act=tf.nn.sigmoid, dropout=True, name_='1'))\n",
    "\n",
    "        self.fc_layers.append(fc_layer(input_dim=FLAGS.node_num, output_dim=FLAGS.node_num, placeholders=self.placeholders,\n",
    "                                       act=tf.nn.softmax, dropout=True, name_='2'))\n",
    "\n",
    "    def _loss(self):\n",
    "        for var in self.gat_layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay*tf.nn.l2_loss(var)\n",
    "\n",
    "\n",
    "        self.loss += tf.reduce_mean(tf.losses.log_loss(labels=self.placeholders['labels'], predictions=self.outputs))\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = accuracy(self.outputs, self.placeholders['labels'])\n",
    "\n",
    "    def predict(self):\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccc1c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '/home/celery/Documents/Research/dataset/Outputs/'\n",
    "data_folder = os.path.join(root_folder, 'cpac/filt_noglobal')\n",
    "subject_IDs = np.genfromtxt('/home/celery/Documents/Research/AI_Tsinghua_Au_2025/Code/IDs/under15_short_IDs.txt', dtype=str)\n",
    "subject_IDs = subject_IDs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e9f24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get label\n",
    "label_dict = Reader.get_label(subject_IDs)\n",
    "label_list = np.array([int(label_dict[x]) for x in subject_IDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a5897ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/celery/Documents/Research/dataset/Outputs/cpac/filt_global/mat/'\n",
    "def load_connectivity(subject_list, kind, atlas_name = 'ho'):\n",
    "    all_networks = []\n",
    "    for subject in subject_list:\n",
    "        fl = os.path.join(data_folder,\n",
    "                          subject + \"_\" + atlas_name + \"_\" + kind + \".mat\")\n",
    "        matrix = sio.loadmat(fl)['connectivity']\n",
    "        if atlas_name == 'ho':\n",
    "            matrix = np.delete(matrix, 82, axis=0)\n",
    "            matrix = np.delete(matrix, 82, axis=1)\n",
    "        all_networks.append(matrix)\n",
    "    all_networks=np.array(all_networks)\n",
    "    return all_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e9ebe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getconn_vector(subject_name0, kind, atlas):\n",
    "    subject_name = np.array(subject_name0)\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    conn_array = load_connectivity(subject_name, kind, atlas)\n",
    "    data_x = np.array(conn_array)\n",
    "\n",
    "    for subname in subject_name:\n",
    "        data_y.append([int(label_dict[subname])])\n",
    "\n",
    "    data_y = np.array(data_y)\n",
    "    return data_x, data_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bac84ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = getconn_vector(subject_IDs, \"correlation\", \"ho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f64f3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_x = map(abs, X)\n",
    "adjs = np.array(list(abs_x))\n",
    "features = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051c1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders -> moved into the loop\n",
    "placeholders = {\n",
    "    'adj': tf.placeholder(tf.float32, shape=(None,FLAGS.node_num,FLAGS.node_num)),\n",
    "    'features': tf.placeholder(tf.float32, shape=(None,FLAGS.node_num,FLAGS.node_num)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, 1)),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'in_drop': tf.placeholder_with_default(0., shape=()),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c8107b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feed_dict(features, support, labels, placeholders):\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['adj']: support})\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b96ebdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1110ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(adjs, features, y):\n",
    "    shuffle_ix = np.random.permutation(np.arange(len(y)))\n",
    "    adjs = adjs[shuffle_ix]\n",
    "    features = features[shuffle_ix]\n",
    "    y = y[shuffle_ix]\n",
    "    return adjs, features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "760df4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_index = []\n",
    "all_train_index = []\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(subject_IDs, label_list):\n",
    "    all_train_index.append(train_index)\n",
    "    all_test_index.append(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7a9ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c2775",
   "metadata": {},
   "source": [
    "Below is the loop all 5 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b114854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "full_results = []\n",
    "\n",
    "l1, l2 = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e966d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n",
      "test shape: (92, 110, 110) support shape: (365, 110, 110)\n",
      "y_test [[1]\n",
      " [1]\n",
      " [0]]\n",
      "WARNING:tensorflow:From <ipython-input-7-bf741a9a2ac7>:59: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "<unknown>\n",
      "Epoch: 0001 train_loss= 0.74715 train_acc= 0.50278 test_loss= 0.74475 test_acc= 0.60477 time= 2.71836\n",
      "Epoch: 0002 train_loss= 0.74468 train_acc= 0.53056 test_loss= 0.74195 test_acc= 0.59390 time= 2.36424\n",
      "Epoch: 0003 train_loss= 0.74250 train_acc= 0.54722 test_loss= 0.73974 test_acc= 0.57277 time= 2.30639\n",
      "Epoch: 0004 train_loss= 0.74063 train_acc= 0.55278 test_loss= 0.73804 test_acc= 0.55435 time= 2.31830\n",
      "Epoch: 0005 train_loss= 0.73902 train_acc= 0.55278 test_loss= 0.73663 test_acc= 0.55435 time= 2.40484\n",
      "Epoch: 0006 train_loss= 0.73750 train_acc= 0.55556 test_loss= 0.73537 test_acc= 0.55435 time= 2.45352\n",
      "Epoch: 0007 train_loss= 0.73607 train_acc= 0.55833 test_loss= 0.73420 test_acc= 0.55435 time= 2.53175\n",
      "Epoch: 0008 train_loss= 0.73468 train_acc= 0.55833 test_loss= 0.73309 test_acc= 0.55435 time= 2.68953\n",
      "Epoch: 0009 train_loss= 0.73332 train_acc= 0.55833 test_loss= 0.73203 test_acc= 0.55435 time= 2.53982\n",
      "Epoch: 0010 train_loss= 0.73196 train_acc= 0.55833 test_loss= 0.73096 test_acc= 0.55435 time= 2.44501\n",
      "Epoch: 0011 train_loss= 0.73058 train_acc= 0.55833 test_loss= 0.72990 test_acc= 0.55435 time= 2.47532\n",
      "Epoch: 0012 train_loss= 0.72916 train_acc= 0.55833 test_loss= 0.72882 test_acc= 0.55435 time= 2.48279\n",
      "Epoch: 0013 train_loss= 0.72767 train_acc= 0.55833 test_loss= 0.72768 test_acc= 0.55435 time= 2.47876\n",
      "Epoch: 0014 train_loss= 0.72605 train_acc= 0.55833 test_loss= 0.72650 test_acc= 0.55435 time= 2.47326\n",
      "Epoch: 0015 train_loss= 0.72428 train_acc= 0.56111 test_loss= 0.72526 test_acc= 0.55435 time= 2.51286\n",
      "Epoch: 0016 train_loss= 0.72239 train_acc= 0.56111 test_loss= 0.72389 test_acc= 0.55435 time= 2.52339\n",
      "Epoch: 0017 train_loss= 0.72039 train_acc= 0.56944 test_loss= 0.72242 test_acc= 0.55676 time= 2.55394\n",
      "Epoch: 0018 train_loss= 0.71826 train_acc= 0.56944 test_loss= 0.72083 test_acc= 0.56944 time= 2.51505\n",
      "Epoch: 0019 train_loss= 0.71597 train_acc= 0.56944 test_loss= 0.71910 test_acc= 0.57790 time= 2.63362\n",
      "Epoch: 0020 train_loss= 0.71349 train_acc= 0.58056 test_loss= 0.71728 test_acc= 0.59118 time= 2.71790\n",
      "Epoch: 0021 train_loss= 0.71077 train_acc= 0.58611 test_loss= 0.71529 test_acc= 0.60870 time= 2.60551\n",
      "Epoch: 0022 train_loss= 0.70781 train_acc= 0.59722 test_loss= 0.71317 test_acc= 0.60839 time= 2.56045\n",
      "Epoch: 0023 train_loss= 0.70463 train_acc= 0.61389 test_loss= 0.71095 test_acc= 0.62440 time= 2.63092\n",
      "Epoch: 0024 train_loss= 0.70127 train_acc= 0.61389 test_loss= 0.70863 test_acc= 0.63678 time= 2.68908\n",
      "Epoch: 0025 train_loss= 0.69769 train_acc= 0.62222 test_loss= 0.70629 test_acc= 0.62772 time= 2.71408\n",
      "Epoch: 0026 train_loss= 0.69387 train_acc= 0.62778 test_loss= 0.70388 test_acc= 0.62862 time= 2.67832\n",
      "Epoch: 0027 train_loss= 0.68991 train_acc= 0.63611 test_loss= 0.70142 test_acc= 0.64493 time= 2.64858\n",
      "Epoch: 0028 train_loss= 0.68579 train_acc= 0.65278 test_loss= 0.69895 test_acc= 0.64312 time= 2.67098\n",
      "Epoch: 0029 train_loss= 0.68152 train_acc= 0.65833 test_loss= 0.69646 test_acc= 0.64251 time= 2.70990\n",
      "Epoch: 0030 train_loss= 0.67715 train_acc= 0.68333 test_loss= 0.69390 test_acc= 0.64312 time= 2.72144\n",
      "Epoch: 0031 train_loss= 0.67271 train_acc= 0.68889 test_loss= 0.69139 test_acc= 0.62953 time= 2.70826\n",
      "Epoch: 0032 train_loss= 0.66825 train_acc= 0.69167 test_loss= 0.68901 test_acc= 0.61232 time= 2.66235\n",
      "Epoch: 0033 train_loss= 0.66374 train_acc= 0.70278 test_loss= 0.68680 test_acc= 0.59511 time= 2.70939\n",
      "Epoch: 0034 train_loss= 0.65924 train_acc= 0.71111 test_loss= 0.68471 test_acc= 0.58031 time= 2.72887\n",
      "Epoch: 0035 train_loss= 0.65471 train_acc= 0.71389 test_loss= 0.68276 test_acc= 0.57699 time= 2.72804\n",
      "Epoch: 0036 train_loss= 0.65023 train_acc= 0.72222 test_loss= 0.68104 test_acc= 0.57669 time= 2.68726\n",
      "Epoch: 0037 train_loss= 0.64577 train_acc= 0.73056 test_loss= 0.67954 test_acc= 0.57911 time= 2.69155\n",
      "Epoch: 0038 train_loss= 0.64131 train_acc= 0.73333 test_loss= 0.67821 test_acc= 0.58998 time= 2.71648\n",
      "Epoch: 0039 train_loss= 0.63690 train_acc= 0.73333 test_loss= 0.67698 test_acc= 0.59813 time= 2.74757\n",
      "Epoch: 0040 train_loss= 0.63253 train_acc= 0.73333 test_loss= 0.67591 test_acc= 0.60236 time= 2.72914\n",
      "Epoch: 0041 train_loss= 0.62824 train_acc= 0.73611 test_loss= 0.67497 test_acc= 0.60507 time= 2.75431\n",
      "Epoch: 0042 train_loss= 0.62400 train_acc= 0.73611 test_loss= 0.67420 test_acc= 0.61141 time= 2.71827\n",
      "Epoch: 0043 train_loss= 0.61978 train_acc= 0.73889 test_loss= 0.67362 test_acc= 0.61775 time= 2.73516\n",
      "Epoch: 0044 train_loss= 0.61560 train_acc= 0.73889 test_loss= 0.67313 test_acc= 0.62168 time= 2.77046\n",
      "Epoch: 0045 train_loss= 0.61143 train_acc= 0.74167 test_loss= 0.67282 test_acc= 0.62802 time= 2.74646\n",
      "Epoch: 0046 train_loss= 0.60729 train_acc= 0.74444 test_loss= 0.67256 test_acc= 0.63466 time= 2.72864\n",
      "Epoch: 0047 train_loss= 0.60318 train_acc= 0.75000 test_loss= 0.67235 test_acc= 0.63889 time= 2.74724\n",
      "Epoch: 0048 train_loss= 0.59913 train_acc= 0.75278 test_loss= 0.67222 test_acc= 0.63889 time= 2.71935\n",
      "Epoch: 0049 train_loss= 0.59517 train_acc= 0.75278 test_loss= 0.67219 test_acc= 0.63798 time= 2.73620\n",
      "Epoch: 0050 train_loss= 0.59123 train_acc= 0.75556 test_loss= 0.67224 test_acc= 0.63829 time= 2.76175\n",
      "Epoch: 0051 train_loss= 0.58736 train_acc= 0.75833 test_loss= 0.67236 test_acc= 0.64221 time= 2.80036\n",
      "Epoch: 0052 train_loss= 0.58352 train_acc= 0.75833 test_loss= 0.67258 test_acc= 0.64583 time= 2.78440\n",
      "Epoch: 0053 train_loss= 0.57974 train_acc= 0.75556 test_loss= 0.67288 test_acc= 0.65187 time= 2.76996\n",
      "Epoch: 0054 train_loss= 0.57601 train_acc= 0.75833 test_loss= 0.67320 test_acc= 0.65519 time= 2.72469\n",
      "Epoch: 0055 train_loss= 0.57233 train_acc= 0.75833 test_loss= 0.67361 test_acc= 0.65610 time= 2.77667\n",
      "early_stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.67489 accuracy= 0.65217 time= 0.057382\n",
      "accuracy, sensivity, specificity, fscore, auc: 0.6521739130434783 0.6341463414634146 0.6666666666666666 0.6190476190476191 0.6924916307986609\n",
      "\n",
      "=== Fold 1 ===\n",
      "test shape: (92, 110, 110) support shape: (365, 110, 110)\n",
      "y_test [[1]\n",
      " [0]\n",
      " [0]]\n",
      "<unknown>\n",
      "Epoch: 0001 train_loss= 0.74727 train_acc= 0.48056 test_loss= 0.74730 test_acc= 0.53140 time= 3.10249\n",
      "Epoch: 0002 train_loss= 0.74512 train_acc= 0.55000 test_loss= 0.74562 test_acc= 0.55767 time= 2.59082\n",
      "Epoch: 0003 train_loss= 0.74340 train_acc= 0.55278 test_loss= 0.74424 test_acc= 0.56129 time= 2.63450\n",
      "Epoch: 0004 train_loss= 0.74181 train_acc= 0.55278 test_loss= 0.74307 test_acc= 0.55435 time= 2.64195\n",
      "Epoch: 0005 train_loss= 0.74031 train_acc= 0.55278 test_loss= 0.74189 test_acc= 0.55435 time= 2.66545\n",
      "Epoch: 0006 train_loss= 0.73883 train_acc= 0.55278 test_loss= 0.74077 test_acc= 0.55435 time= 2.66910\n",
      "Epoch: 0007 train_loss= 0.73739 train_acc= 0.55278 test_loss= 0.73963 test_acc= 0.55435 time= 2.71428\n",
      "Epoch: 0008 train_loss= 0.73591 train_acc= 0.55278 test_loss= 0.73850 test_acc= 0.55435 time= 2.70308\n",
      "Epoch: 0009 train_loss= 0.73441 train_acc= 0.55278 test_loss= 0.73735 test_acc= 0.55435 time= 2.78185\n",
      "Epoch: 0010 train_loss= 0.73290 train_acc= 0.55278 test_loss= 0.73623 test_acc= 0.55435 time= 2.84504\n",
      "Epoch: 0011 train_loss= 0.73143 train_acc= 0.55278 test_loss= 0.73516 test_acc= 0.55435 time= 2.86745\n",
      "Epoch: 0012 train_loss= 0.72994 train_acc= 0.55278 test_loss= 0.73411 test_acc= 0.55435 time= 2.87839\n",
      "Epoch: 0013 train_loss= 0.72844 train_acc= 0.55278 test_loss= 0.73306 test_acc= 0.55435 time= 2.90839\n",
      "Epoch: 0014 train_loss= 0.72690 train_acc= 0.55278 test_loss= 0.73208 test_acc= 0.55435 time= 2.98216\n",
      "Epoch: 0015 train_loss= 0.72522 train_acc= 0.55278 test_loss= 0.73111 test_acc= 0.55435 time= 2.86750\n",
      "Epoch: 0016 train_loss= 0.72342 train_acc= 0.55278 test_loss= 0.73011 test_acc= 0.55435 time= 2.91181\n",
      "Epoch: 0017 train_loss= 0.72145 train_acc= 0.55278 test_loss= 0.72905 test_acc= 0.55435 time= 2.97395\n",
      "Epoch: 0018 train_loss= 0.71936 train_acc= 0.55278 test_loss= 0.72796 test_acc= 0.55435 time= 2.85867\n",
      "Epoch: 0019 train_loss= 0.71719 train_acc= 0.55278 test_loss= 0.72676 test_acc= 0.55435 time= 2.91068\n",
      "Epoch: 0020 train_loss= 0.71486 train_acc= 0.55278 test_loss= 0.72551 test_acc= 0.55435 time= 2.93032\n",
      "Epoch: 0021 train_loss= 0.71239 train_acc= 0.55833 test_loss= 0.72422 test_acc= 0.55465 time= 2.84674\n",
      "Epoch: 0022 train_loss= 0.70981 train_acc= 0.55833 test_loss= 0.72289 test_acc= 0.56522 time= 2.88764\n",
      "Epoch: 0023 train_loss= 0.70699 train_acc= 0.56667 test_loss= 0.72152 test_acc= 0.56522 time= 2.90061\n",
      "Epoch: 0024 train_loss= 0.70407 train_acc= 0.57222 test_loss= 0.72013 test_acc= 0.56763 time= 2.82109\n",
      "Epoch: 0025 train_loss= 0.70095 train_acc= 0.58056 test_loss= 0.71861 test_acc= 0.56733 time= 2.85134\n",
      "Epoch: 0026 train_loss= 0.69789 train_acc= 0.59167 test_loss= 0.71704 test_acc= 0.57729 time= 2.87152\n",
      "Epoch: 0027 train_loss= 0.69473 train_acc= 0.60000 test_loss= 0.71553 test_acc= 0.58696 time= 2.84695\n",
      "Epoch: 0028 train_loss= 0.69154 train_acc= 0.59722 test_loss= 0.71408 test_acc= 0.57609 time= 2.86287\n",
      "Epoch: 0029 train_loss= 0.68829 train_acc= 0.61389 test_loss= 0.71267 test_acc= 0.56824 time= 2.88668\n",
      "Epoch: 0030 train_loss= 0.68502 train_acc= 0.63611 test_loss= 0.71138 test_acc= 0.57397 time= 2.84572\n",
      "Epoch: 0031 train_loss= 0.68174 train_acc= 0.64167 test_loss= 0.71014 test_acc= 0.57971 time= 2.86841\n",
      "Epoch: 0032 train_loss= 0.67855 train_acc= 0.65278 test_loss= 0.70892 test_acc= 0.57820 time= 2.88573\n",
      "Epoch: 0033 train_loss= 0.67525 train_acc= 0.66944 test_loss= 0.70769 test_acc= 0.56884 time= 2.88527\n",
      "Epoch: 0034 train_loss= 0.67200 train_acc= 0.67500 test_loss= 0.70650 test_acc= 0.57609 time= 2.89101\n",
      "Epoch: 0035 train_loss= 0.66874 train_acc= 0.68611 test_loss= 0.70535 test_acc= 0.58243 time= 2.88808\n",
      "Epoch: 0036 train_loss= 0.66551 train_acc= 0.69167 test_loss= 0.70428 test_acc= 0.59028 time= 2.88460\n",
      "Epoch: 0037 train_loss= 0.66221 train_acc= 0.69722 test_loss= 0.70321 test_acc= 0.58424 time= 2.87124\n",
      "Epoch: 0038 train_loss= 0.65891 train_acc= 0.69444 test_loss= 0.70215 test_acc= 0.58062 time= 2.87214\n",
      "Epoch: 0039 train_loss= 0.65565 train_acc= 0.70000 test_loss= 0.70114 test_acc= 0.57458 time= 2.91587\n",
      "Epoch: 0040 train_loss= 0.65226 train_acc= 0.70278 test_loss= 0.70021 test_acc= 0.57065 time= 2.95493\n",
      "Epoch: 0041 train_loss= 0.64900 train_acc= 0.70278 test_loss= 0.69935 test_acc= 0.57095 time= 2.87242\n",
      "Epoch: 0042 train_loss= 0.64556 train_acc= 0.71111 test_loss= 0.69853 test_acc= 0.57246 time= 2.85158\n",
      "Epoch: 0043 train_loss= 0.64232 train_acc= 0.70833 test_loss= 0.69781 test_acc= 0.56552 time= 2.88144\n",
      "Epoch: 0044 train_loss= 0.63894 train_acc= 0.71944 test_loss= 0.69711 test_acc= 0.57911 time= 2.84909\n",
      "Epoch: 0045 train_loss= 0.63567 train_acc= 0.71944 test_loss= 0.69644 test_acc= 0.58182 time= 2.88443\n",
      "Epoch: 0046 train_loss= 0.63238 train_acc= 0.71667 test_loss= 0.69584 test_acc= 0.57880 time= 2.94759\n",
      "Epoch: 0047 train_loss= 0.62915 train_acc= 0.72222 test_loss= 0.69528 test_acc= 0.57699 time= 2.92576\n",
      "Epoch: 0048 train_loss= 0.62586 train_acc= 0.72500 test_loss= 0.69474 test_acc= 0.57518 time= 2.94782\n",
      "Epoch: 0049 train_loss= 0.62265 train_acc= 0.72778 test_loss= 0.69428 test_acc= 0.56884 time= 2.96114\n",
      "Epoch: 0050 train_loss= 0.61946 train_acc= 0.73056 test_loss= 0.69387 test_acc= 0.56824 time= 2.89429\n",
      "Epoch: 0051 train_loss= 0.61620 train_acc= 0.73056 test_loss= 0.69351 test_acc= 0.56884 time= 2.88558\n",
      "Epoch: 0052 train_loss= 0.61305 train_acc= 0.73056 test_loss= 0.69321 test_acc= 0.57307 time= 2.84540\n",
      "Epoch: 0053 train_loss= 0.60989 train_acc= 0.73056 test_loss= 0.69298 test_acc= 0.57397 time= 2.85671\n",
      "Epoch: 0054 train_loss= 0.60676 train_acc= 0.73056 test_loss= 0.69276 test_acc= 0.57397 time= 2.92587\n",
      "Epoch: 0055 train_loss= 0.60363 train_acc= 0.73333 test_loss= 0.69260 test_acc= 0.57367 time= 2.94173\n",
      "Epoch: 0056 train_loss= 0.60053 train_acc= 0.73889 test_loss= 0.69247 test_acc= 0.57428 time= 2.93104\n",
      "Epoch: 0057 train_loss= 0.59748 train_acc= 0.74167 test_loss= 0.69237 test_acc= 0.58001 time= 2.94211\n",
      "Epoch: 0058 train_loss= 0.59441 train_acc= 0.73889 test_loss= 0.69234 test_acc= 0.58152 time= 2.95048\n",
      "Epoch: 0059 train_loss= 0.59134 train_acc= 0.74444 test_loss= 0.69233 test_acc= 0.58303 time= 2.89029\n",
      "Epoch: 0060 train_loss= 0.58829 train_acc= 0.74722 test_loss= 0.69236 test_acc= 0.58454 time= 2.90305\n",
      "Epoch: 0061 train_loss= 0.58533 train_acc= 0.74722 test_loss= 0.69243 test_acc= 0.59209 time= 2.91620\n",
      "Epoch: 0062 train_loss= 0.58226 train_acc= 0.75278 test_loss= 0.69252 test_acc= 0.59722 time= 2.94563\n",
      "Epoch: 0063 train_loss= 0.57932 train_acc= 0.76111 test_loss= 0.69269 test_acc= 0.60024 time= 2.93004\n",
      "Epoch: 0064 train_loss= 0.57627 train_acc= 0.76667 test_loss= 0.69288 test_acc= 0.60417 time= 2.92171\n",
      "early_stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.69276 accuracy= 0.60870 time= 0.061235\n",
      "accuracy, sensivity, specificity, fscore, auc: 0.6086956521739131 0.4878048780487805 0.7058823529411765 0.5263157894736842 0.6484935437589671\n",
      "\n",
      "=== Fold 2 ===\n",
      "test shape: (91, 110, 110) support shape: (366, 110, 110)\n",
      "y_test [[0]\n",
      " [0]\n",
      " [0]]\n",
      "<unknown>\n",
      "Epoch: 0001 train_loss= 0.75027 train_acc= 0.44955 test_loss= 0.75214 test_acc= 0.41461 time= 3.18662\n",
      "Epoch: 0002 train_loss= 0.74722 train_acc= 0.48468 test_loss= 0.74947 test_acc= 0.44906 time= 2.68922\n",
      "Epoch: 0003 train_loss= 0.74550 train_acc= 0.55135 test_loss= 0.74804 test_acc= 0.49748 time= 2.77025\n",
      "Epoch: 0004 train_loss= 0.74448 train_acc= 0.57838 test_loss= 0.74720 test_acc= 0.50549 time= 2.80051\n",
      "Epoch: 0005 train_loss= 0.74358 train_acc= 0.60270 test_loss= 0.74647 test_acc= 0.49807 time= 2.83287\n",
      "Epoch: 0006 train_loss= 0.74269 train_acc= 0.61351 test_loss= 0.74584 test_acc= 0.49540 time= 2.82636\n",
      "Epoch: 0007 train_loss= 0.74180 train_acc= 0.62162 test_loss= 0.74521 test_acc= 0.49480 time= 2.86115\n",
      "Epoch: 0008 train_loss= 0.74086 train_acc= 0.62432 test_loss= 0.74456 test_acc= 0.50787 time= 2.86592\n",
      "Epoch: 0009 train_loss= 0.73991 train_acc= 0.64054 test_loss= 0.74394 test_acc= 0.52361 time= 2.92524\n",
      "Epoch: 0010 train_loss= 0.73894 train_acc= 0.64595 test_loss= 0.74329 test_acc= 0.53074 time= 2.92184\n",
      "Epoch: 0011 train_loss= 0.73792 train_acc= 0.65135 test_loss= 0.74266 test_acc= 0.53995 time= 2.88797\n",
      "Epoch: 0012 train_loss= 0.73688 train_acc= 0.65946 test_loss= 0.74201 test_acc= 0.52777 time= 2.92245\n",
      "Epoch: 0013 train_loss= 0.73578 train_acc= 0.67027 test_loss= 0.74140 test_acc= 0.51916 time= 2.95659\n",
      "Epoch: 0014 train_loss= 0.73461 train_acc= 0.66486 test_loss= 0.74079 test_acc= 0.52628 time= 2.98664\n",
      "Epoch: 0015 train_loss= 0.73335 train_acc= 0.67027 test_loss= 0.74017 test_acc= 0.52747 time= 2.96226\n",
      "Epoch: 0016 train_loss= 0.73199 train_acc= 0.68108 test_loss= 0.73948 test_acc= 0.52599 time= 2.88668\n",
      "Epoch: 0017 train_loss= 0.73051 train_acc= 0.68919 test_loss= 0.73877 test_acc= 0.52599 time= 2.89870\n",
      "Epoch: 0018 train_loss= 0.72892 train_acc= 0.68919 test_loss= 0.73808 test_acc= 0.52896 time= 2.91638\n",
      "Epoch: 0019 train_loss= 0.72716 train_acc= 0.68108 test_loss= 0.73743 test_acc= 0.52539 time= 3.01652\n",
      "Epoch: 0020 train_loss= 0.72527 train_acc= 0.70090 test_loss= 0.73674 test_acc= 0.53312 time= 3.01868\n",
      "Epoch: 0021 train_loss= 0.72316 train_acc= 0.71171 test_loss= 0.73607 test_acc= 0.53282 time= 3.05703\n",
      "Epoch: 0022 train_loss= 0.72082 train_acc= 0.70360 test_loss= 0.73534 test_acc= 0.54024 time= 3.06738\n",
      "Epoch: 0023 train_loss= 0.71824 train_acc= 0.70360 test_loss= 0.73464 test_acc= 0.55004 time= 3.18169\n",
      "Epoch: 0024 train_loss= 0.71548 train_acc= 0.71171 test_loss= 0.73380 test_acc= 0.55539 time= 3.30762\n",
      "Epoch: 0025 train_loss= 0.71248 train_acc= 0.71441 test_loss= 0.73268 test_acc= 0.55658 time= 3.12578\n",
      "Epoch: 0026 train_loss= 0.70929 train_acc= 0.71171 test_loss= 0.73151 test_acc= 0.56163 time= 3.03080\n",
      "Epoch: 0027 train_loss= 0.70590 train_acc= 0.71712 test_loss= 0.73030 test_acc= 0.56282 time= 2.99247\n",
      "Epoch: 0028 train_loss= 0.70245 train_acc= 0.71982 test_loss= 0.72913 test_acc= 0.57113 time= 2.92169\n",
      "Epoch: 0029 train_loss= 0.69883 train_acc= 0.72252 test_loss= 0.72794 test_acc= 0.57143 time= 2.92272\n",
      "Epoch: 0030 train_loss= 0.69517 train_acc= 0.72793 test_loss= 0.72671 test_acc= 0.57143 time= 2.89723\n",
      "Epoch: 0031 train_loss= 0.69145 train_acc= 0.72973 test_loss= 0.72542 test_acc= 0.57529 time= 2.92447\n",
      "Epoch: 0032 train_loss= 0.68768 train_acc= 0.73514 test_loss= 0.72411 test_acc= 0.57885 time= 2.96501\n",
      "Epoch: 0033 train_loss= 0.68386 train_acc= 0.73784 test_loss= 0.72276 test_acc= 0.57737 time= 2.92068\n",
      "Epoch: 0034 train_loss= 0.67999 train_acc= 0.73784 test_loss= 0.72142 test_acc= 0.57529 time= 2.93946\n",
      "Epoch: 0035 train_loss= 0.67612 train_acc= 0.74054 test_loss= 0.72007 test_acc= 0.58450 time= 2.94506\n",
      "Epoch: 0036 train_loss= 0.67217 train_acc= 0.73784 test_loss= 0.71860 test_acc= 0.59044 time= 2.95229\n",
      "Epoch: 0037 train_loss= 0.66818 train_acc= 0.74054 test_loss= 0.71711 test_acc= 0.59222 time= 2.95239\n",
      "Epoch: 0038 train_loss= 0.66422 train_acc= 0.74865 test_loss= 0.71568 test_acc= 0.59252 time= 2.96297\n",
      "Epoch: 0039 train_loss= 0.66023 train_acc= 0.75405 test_loss= 0.71429 test_acc= 0.59697 time= 2.96762\n",
      "Epoch: 0040 train_loss= 0.65628 train_acc= 0.75405 test_loss= 0.71294 test_acc= 0.60053 time= 2.94787\n",
      "Epoch: 0041 train_loss= 0.65228 train_acc= 0.75135 test_loss= 0.71154 test_acc= 0.60291 time= 3.00132\n",
      "Epoch: 0042 train_loss= 0.64838 train_acc= 0.75405 test_loss= 0.71020 test_acc= 0.60499 time= 3.02041\n",
      "Epoch: 0043 train_loss= 0.64445 train_acc= 0.75946 test_loss= 0.70884 test_acc= 0.60440 time= 2.96846\n",
      "Epoch: 0044 train_loss= 0.64062 train_acc= 0.76216 test_loss= 0.70753 test_acc= 0.60855 time= 3.00362\n",
      "Epoch: 0045 train_loss= 0.63683 train_acc= 0.75946 test_loss= 0.70626 test_acc= 0.62192 time= 2.96684\n",
      "Epoch: 0046 train_loss= 0.63310 train_acc= 0.75676 test_loss= 0.70507 test_acc= 0.63083 time= 2.93073\n",
      "Epoch: 0047 train_loss= 0.62933 train_acc= 0.75946 test_loss= 0.70391 test_acc= 0.63617 time= 2.96477\n",
      "Epoch: 0048 train_loss= 0.62569 train_acc= 0.75946 test_loss= 0.70276 test_acc= 0.63736 time= 2.93252\n",
      "Epoch: 0049 train_loss= 0.62206 train_acc= 0.76216 test_loss= 0.70167 test_acc= 0.63707 time= 3.01351\n",
      "Epoch: 0050 train_loss= 0.61851 train_acc= 0.75676 test_loss= 0.70065 test_acc= 0.63469 time= 3.01908\n",
      "Epoch: 0051 train_loss= 0.61498 train_acc= 0.76216 test_loss= 0.69971 test_acc= 0.63053 time= 2.93422\n",
      "Epoch: 0052 train_loss= 0.61152 train_acc= 0.76486 test_loss= 0.69878 test_acc= 0.62875 time= 2.93417\n",
      "Epoch: 0053 train_loss= 0.60810 train_acc= 0.77027 test_loss= 0.69791 test_acc= 0.62637 time= 2.97243\n",
      "Epoch: 0054 train_loss= 0.60474 train_acc= 0.77568 test_loss= 0.69711 test_acc= 0.62637 time= 2.97926\n",
      "Epoch: 0055 train_loss= 0.60144 train_acc= 0.77568 test_loss= 0.69641 test_acc= 0.63202 time= 2.97452\n",
      "Epoch: 0056 train_loss= 0.59825 train_acc= 0.77568 test_loss= 0.69581 test_acc= 0.63617 time= 2.95106\n",
      "Epoch: 0057 train_loss= 0.59516 train_acc= 0.78108 test_loss= 0.69526 test_acc= 0.63736 time= 2.93649\n",
      "Epoch: 0058 train_loss= 0.59203 train_acc= 0.78649 test_loss= 0.69472 test_acc= 0.63736 time= 2.98368\n",
      "Epoch: 0059 train_loss= 0.58901 train_acc= 0.78919 test_loss= 0.69425 test_acc= 0.63736 time= 2.93296\n",
      "Epoch: 0060 train_loss= 0.58611 train_acc= 0.78919 test_loss= 0.69381 test_acc= 0.64093 time= 2.90306\n",
      "Epoch: 0061 train_loss= 0.58321 train_acc= 0.78919 test_loss= 0.69339 test_acc= 0.64776 time= 2.91754\n",
      "Epoch: 0062 train_loss= 0.58034 train_acc= 0.78919 test_loss= 0.69305 test_acc= 0.64895 time= 2.93235\n",
      "Epoch: 0063 train_loss= 0.57761 train_acc= 0.79189 test_loss= 0.69285 test_acc= 0.65192 time= 2.93962\n",
      "Epoch: 0064 train_loss= 0.57488 train_acc= 0.79189 test_loss= 0.69266 test_acc= 0.65221 time= 2.96112\n",
      "Epoch: 0065 train_loss= 0.57215 train_acc= 0.79730 test_loss= 0.69252 test_acc= 0.65281 time= 2.99102\n",
      "Epoch: 0066 train_loss= 0.56951 train_acc= 0.80000 test_loss= 0.69246 test_acc= 0.65518 time= 3.00113\n",
      "Epoch: 0067 train_loss= 0.56691 train_acc= 0.80000 test_loss= 0.69235 test_acc= 0.65607 time= 2.96042\n",
      "Epoch: 0068 train_loss= 0.56433 train_acc= 0.80270 test_loss= 0.69226 test_acc= 0.65756 time= 3.00581\n",
      "Epoch: 0069 train_loss= 0.56175 train_acc= 0.80270 test_loss= 0.69226 test_acc= 0.65934 time= 2.95784\n",
      "Epoch: 0070 train_loss= 0.55930 train_acc= 0.80270 test_loss= 0.69225 test_acc= 0.65934 time= 2.97104\n",
      "Epoch: 0071 train_loss= 0.55681 train_acc= 0.80270 test_loss= 0.69229 test_acc= 0.65934 time= 2.94326\n",
      "Epoch: 0072 train_loss= 0.55439 train_acc= 0.80270 test_loss= 0.69239 test_acc= 0.65934 time= 2.97468\n",
      "Epoch: 0073 train_loss= 0.55204 train_acc= 0.80541 test_loss= 0.69250 test_acc= 0.65934 time= 2.97164\n",
      "Epoch: 0074 train_loss= 0.54963 train_acc= 0.80811 test_loss= 0.69266 test_acc= 0.65934 time= 2.93475\n",
      "Epoch: 0075 train_loss= 0.54730 train_acc= 0.80811 test_loss= 0.69284 test_acc= 0.65964 time= 2.96627\n",
      "early_stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.69271 accuracy= 0.67033 time= 0.059865\n",
      "accuracy, sensivity, specificity, fscore, auc: 0.6703296703296703 0.5853658536585366 0.74 0.6153846153846153 0.6682926829268293\n",
      "\n",
      "=== Fold 3 ===\n",
      "test shape: (91, 110, 110) support shape: (366, 110, 110)\n",
      "y_test [[1]\n",
      " [0]\n",
      " [1]]\n",
      "<unknown>\n",
      "Epoch: 0001 train_loss= 0.74669 train_acc= 0.50090 test_loss= 0.74667 test_acc= 0.48055 time= 3.24345\n",
      "Epoch: 0002 train_loss= 0.74519 train_acc= 0.53604 test_loss= 0.74541 test_acc= 0.51827 time= 2.73143\n",
      "Epoch: 0003 train_loss= 0.74389 train_acc= 0.55045 test_loss= 0.74423 test_acc= 0.54648 time= 2.77590\n",
      "Epoch: 0004 train_loss= 0.74268 train_acc= 0.55946 test_loss= 0.74310 test_acc= 0.54559 time= 2.88482\n",
      "Epoch: 0005 train_loss= 0.74146 train_acc= 0.56486 test_loss= 0.74204 test_acc= 0.53846 time= 2.86545\n",
      "Epoch: 0006 train_loss= 0.74018 train_acc= 0.57027 test_loss= 0.74099 test_acc= 0.53341 time= 2.85239\n",
      "Epoch: 0007 train_loss= 0.73887 train_acc= 0.55946 test_loss= 0.73994 test_acc= 0.52747 time= 2.84483\n",
      "Epoch: 0008 train_loss= 0.73755 train_acc= 0.55676 test_loss= 0.73890 test_acc= 0.52747 time= 2.86837\n",
      "Epoch: 0009 train_loss= 0.73623 train_acc= 0.55676 test_loss= 0.73789 test_acc= 0.52747 time= 2.88302\n",
      "Epoch: 0010 train_loss= 0.73489 train_acc= 0.55676 test_loss= 0.73689 test_acc= 0.52747 time= 2.87156\n",
      "Epoch: 0011 train_loss= 0.73353 train_acc= 0.55676 test_loss= 0.73591 test_acc= 0.52747 time= 2.84791\n",
      "Epoch: 0012 train_loss= 0.73214 train_acc= 0.55676 test_loss= 0.73496 test_acc= 0.52747 time= 2.86178\n",
      "Epoch: 0013 train_loss= 0.73068 train_acc= 0.55405 test_loss= 0.73400 test_acc= 0.52747 time= 2.90252\n",
      "Epoch: 0014 train_loss= 0.72919 train_acc= 0.55405 test_loss= 0.73307 test_acc= 0.52747 time= 2.90950\n",
      "Epoch: 0015 train_loss= 0.72765 train_acc= 0.55405 test_loss= 0.73215 test_acc= 0.52747 time= 2.88656\n",
      "Epoch: 0016 train_loss= 0.72600 train_acc= 0.55405 test_loss= 0.73122 test_acc= 0.52925 time= 2.93862\n",
      "Epoch: 0017 train_loss= 0.72426 train_acc= 0.55405 test_loss= 0.73030 test_acc= 0.53846 time= 2.92154\n",
      "Epoch: 0018 train_loss= 0.72238 train_acc= 0.55676 test_loss= 0.72941 test_acc= 0.53935 time= 2.92617\n",
      "Epoch: 0019 train_loss= 0.72035 train_acc= 0.55676 test_loss= 0.72855 test_acc= 0.54589 time= 2.92399\n",
      "Epoch: 0020 train_loss= 0.71810 train_acc= 0.55946 test_loss= 0.72766 test_acc= 0.54945 time= 2.96588\n",
      "Epoch: 0021 train_loss= 0.71579 train_acc= 0.56216 test_loss= 0.72685 test_acc= 0.54945 time= 2.97950\n",
      "Epoch: 0022 train_loss= 0.71345 train_acc= 0.56216 test_loss= 0.72607 test_acc= 0.54945 time= 2.94517\n",
      "Epoch: 0023 train_loss= 0.71105 train_acc= 0.56216 test_loss= 0.72528 test_acc= 0.54945 time= 2.97832\n",
      "Epoch: 0024 train_loss= 0.70856 train_acc= 0.56757 test_loss= 0.72441 test_acc= 0.54945 time= 2.98945\n",
      "Epoch: 0025 train_loss= 0.70605 train_acc= 0.57027 test_loss= 0.72365 test_acc= 0.54945 time= 2.99230\n",
      "Epoch: 0026 train_loss= 0.70357 train_acc= 0.57027 test_loss= 0.72286 test_acc= 0.54945 time= 3.00711\n",
      "Epoch: 0027 train_loss= 0.70112 train_acc= 0.57297 test_loss= 0.72212 test_acc= 0.54945 time= 3.03506\n",
      "Epoch: 0028 train_loss= 0.69868 train_acc= 0.58378 test_loss= 0.72105 test_acc= 0.54410 time= 2.96710\n",
      "Epoch: 0029 train_loss= 0.69611 train_acc= 0.58919 test_loss= 0.71989 test_acc= 0.55955 time= 3.00332\n",
      "Epoch: 0030 train_loss= 0.69348 train_acc= 0.60270 test_loss= 0.71865 test_acc= 0.55925 time= 2.96674\n",
      "Epoch: 0031 train_loss= 0.69083 train_acc= 0.61622 test_loss= 0.71756 test_acc= 0.56341 time= 3.01856\n",
      "Epoch: 0032 train_loss= 0.68817 train_acc= 0.62432 test_loss= 0.71637 test_acc= 0.58598 time= 2.95758\n",
      "Epoch: 0033 train_loss= 0.68541 train_acc= 0.62973 test_loss= 0.71509 test_acc= 0.60143 time= 3.03440\n",
      "Epoch: 0034 train_loss= 0.68257 train_acc= 0.62162 test_loss= 0.71363 test_acc= 0.60588 time= 3.01529\n",
      "Epoch: 0035 train_loss= 0.67964 train_acc= 0.63514 test_loss= 0.71222 test_acc= 0.60350 time= 3.02835\n",
      "Epoch: 0036 train_loss= 0.67677 train_acc= 0.65135 test_loss= 0.71103 test_acc= 0.59667 time= 2.97689\n",
      "Epoch: 0037 train_loss= 0.67389 train_acc= 0.66396 test_loss= 0.70987 test_acc= 0.58658 time= 2.96899\n",
      "Epoch: 0038 train_loss= 0.67100 train_acc= 0.66937 test_loss= 0.70876 test_acc= 0.57529 time= 2.98158\n",
      "Epoch: 0039 train_loss= 0.66811 train_acc= 0.67207 test_loss= 0.70767 test_acc= 0.57321 time= 2.98034\n",
      "Epoch: 0040 train_loss= 0.66516 train_acc= 0.66667 test_loss= 0.70664 test_acc= 0.58717 time= 2.98087\n",
      "Epoch: 0041 train_loss= 0.66222 train_acc= 0.66396 test_loss= 0.70572 test_acc= 0.59994 time= 2.97541\n",
      "Epoch: 0042 train_loss= 0.65927 train_acc= 0.67207 test_loss= 0.70490 test_acc= 0.59756 time= 3.04221\n",
      "Epoch: 0043 train_loss= 0.65636 train_acc= 0.67928 test_loss= 0.70419 test_acc= 0.59222 time= 2.97127\n",
      "Epoch: 0044 train_loss= 0.65345 train_acc= 0.67387 test_loss= 0.70347 test_acc= 0.58479 time= 3.00315\n",
      "Epoch: 0045 train_loss= 0.65054 train_acc= 0.68198 test_loss= 0.70283 test_acc= 0.58242 time= 2.95612\n",
      "Epoch: 0046 train_loss= 0.64764 train_acc= 0.68468 test_loss= 0.70228 test_acc= 0.58123 time= 2.94662\n",
      "Epoch: 0047 train_loss= 0.64476 train_acc= 0.68468 test_loss= 0.70182 test_acc= 0.58182 time= 2.97236\n",
      "Epoch: 0048 train_loss= 0.64185 train_acc= 0.69009 test_loss= 0.70141 test_acc= 0.58242 time= 3.01145\n",
      "Epoch: 0049 train_loss= 0.63899 train_acc= 0.69279 test_loss= 0.70113 test_acc= 0.58242 time= 2.97493\n",
      "Epoch: 0050 train_loss= 0.63614 train_acc= 0.69009 test_loss= 0.70091 test_acc= 0.58539 time= 2.96982\n",
      "Epoch: 0051 train_loss= 0.63331 train_acc= 0.69279 test_loss= 0.70073 test_acc= 0.58628 time= 3.01978\n",
      "Epoch: 0052 train_loss= 0.63049 train_acc= 0.69279 test_loss= 0.70063 test_acc= 0.59311 time= 2.98621\n",
      "Epoch: 0053 train_loss= 0.62769 train_acc= 0.69820 test_loss= 0.70065 test_acc= 0.59667 time= 3.00016\n",
      "Epoch: 0054 train_loss= 0.62492 train_acc= 0.69820 test_loss= 0.70075 test_acc= 0.60024 time= 3.03965\n",
      "Epoch: 0055 train_loss= 0.62217 train_acc= 0.70090 test_loss= 0.70090 test_acc= 0.60440 time= 2.99476\n",
      "Epoch: 0056 train_loss= 0.61943 train_acc= 0.70360 test_loss= 0.70112 test_acc= 0.60440 time= 3.03162\n",
      "Epoch: 0057 train_loss= 0.61672 train_acc= 0.70631 test_loss= 0.70143 test_acc= 0.60440 time= 2.98452\n",
      "Epoch: 0058 train_loss= 0.61405 train_acc= 0.70901 test_loss= 0.70181 test_acc= 0.60440 time= 2.96763\n",
      "early_stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.70193 accuracy= 0.60440 time= 0.060644\n",
      "accuracy, sensivity, specificity, fscore, auc: 0.6043956043956044 0.43902439024390244 0.74 0.5 0.6351219512195122\n",
      "\n",
      "=== Fold 4 ===\n",
      "test shape: (91, 110, 110) support shape: (366, 110, 110)\n",
      "y_test [[0]\n",
      " [1]\n",
      " [1]]\n",
      "<unknown>\n",
      "Epoch: 0001 train_loss= 0.74864 train_acc= 0.46847 test_loss= 0.74771 test_acc= 0.51173 time= 3.17523\n",
      "Epoch: 0002 train_loss= 0.74603 train_acc= 0.52342 test_loss= 0.74599 test_acc= 0.54440 time= 2.73470\n",
      "Epoch: 0003 train_loss= 0.74422 train_acc= 0.56667 test_loss= 0.74459 test_acc= 0.52955 time= 2.76249\n",
      "Epoch: 0004 train_loss= 0.74270 train_acc= 0.55315 test_loss= 0.74338 test_acc= 0.54262 time= 2.77189\n",
      "Epoch: 0005 train_loss= 0.74129 train_acc= 0.54775 test_loss= 0.74225 test_acc= 0.53935 time= 2.79097\n",
      "Epoch: 0006 train_loss= 0.73987 train_acc= 0.55315 test_loss= 0.74110 test_acc= 0.54797 time= 2.79097\n",
      "Epoch: 0007 train_loss= 0.73845 train_acc= 0.55315 test_loss= 0.73995 test_acc= 0.53846 time= 2.80205\n",
      "Epoch: 0008 train_loss= 0.73697 train_acc= 0.55315 test_loss= 0.73882 test_acc= 0.54024 time= 2.82309\n",
      "Epoch: 0009 train_loss= 0.73553 train_acc= 0.55315 test_loss= 0.73771 test_acc= 0.54856 time= 2.82816\n",
      "Epoch: 0010 train_loss= 0.73398 train_acc= 0.55315 test_loss= 0.73657 test_acc= 0.54826 time= 2.86514\n",
      "Epoch: 0011 train_loss= 0.73236 train_acc= 0.55315 test_loss= 0.73532 test_acc= 0.54559 time= 2.89614\n",
      "Epoch: 0012 train_loss= 0.73048 train_acc= 0.55586 test_loss= 0.73405 test_acc= 0.54678 time= 2.90158\n",
      "Epoch: 0013 train_loss= 0.72864 train_acc= 0.55586 test_loss= 0.73275 test_acc= 0.54826 time= 2.94879\n",
      "Epoch: 0014 train_loss= 0.72675 train_acc= 0.55856 test_loss= 0.73134 test_acc= 0.55331 time= 2.93106\n",
      "Epoch: 0015 train_loss= 0.72470 train_acc= 0.56396 test_loss= 0.72989 test_acc= 0.56074 time= 2.93050\n",
      "Epoch: 0016 train_loss= 0.72258 train_acc= 0.56937 test_loss= 0.72833 test_acc= 0.56579 time= 2.96443\n",
      "Epoch: 0017 train_loss= 0.72033 train_acc= 0.58559 test_loss= 0.72668 test_acc= 0.57202 time= 3.04390\n",
      "Epoch: 0018 train_loss= 0.71787 train_acc= 0.59099 test_loss= 0.72487 test_acc= 0.57262 time= 2.98254\n",
      "Epoch: 0019 train_loss= 0.71527 train_acc= 0.59640 test_loss= 0.72312 test_acc= 0.57321 time= 2.94969\n",
      "Epoch: 0020 train_loss= 0.71255 train_acc= 0.61261 test_loss= 0.72132 test_acc= 0.58212 time= 2.92847\n",
      "Epoch: 0021 train_loss= 0.70969 train_acc= 0.62342 test_loss= 0.71951 test_acc= 0.59133 time= 2.92695\n",
      "Epoch: 0022 train_loss= 0.70671 train_acc= 0.61532 test_loss= 0.71767 test_acc= 0.59459 time= 2.91563\n",
      "Epoch: 0023 train_loss= 0.70349 train_acc= 0.62883 test_loss= 0.71568 test_acc= 0.60410 time= 3.01763\n",
      "Epoch: 0024 train_loss= 0.69999 train_acc= 0.64234 test_loss= 0.71360 test_acc= 0.59133 time= 3.07838\n",
      "Epoch: 0025 train_loss= 0.69631 train_acc= 0.65586 test_loss= 0.71151 test_acc= 0.58331 time= 3.10763\n",
      "Epoch: 0026 train_loss= 0.69253 train_acc= 0.65045 test_loss= 0.70940 test_acc= 0.58925 time= 3.04788\n",
      "Epoch: 0027 train_loss= 0.68865 train_acc= 0.66126 test_loss= 0.70725 test_acc= 0.60053 time= 2.98033\n",
      "Epoch: 0028 train_loss= 0.68465 train_acc= 0.66126 test_loss= 0.70513 test_acc= 0.61628 time= 2.93502\n",
      "Epoch: 0029 train_loss= 0.68051 train_acc= 0.67477 test_loss= 0.70305 test_acc= 0.62103 time= 2.95132\n",
      "Epoch: 0030 train_loss= 0.67627 train_acc= 0.67477 test_loss= 0.70103 test_acc= 0.62162 time= 2.95142\n",
      "Epoch: 0031 train_loss= 0.67201 train_acc= 0.66937 test_loss= 0.69910 test_acc= 0.64004 time= 2.93295\n",
      "Epoch: 0032 train_loss= 0.66775 train_acc= 0.67207 test_loss= 0.69732 test_acc= 0.64568 time= 2.95812\n",
      "Epoch: 0033 train_loss= 0.66344 train_acc= 0.67748 test_loss= 0.69568 test_acc= 0.64568 time= 2.95819\n",
      "Epoch: 0034 train_loss= 0.65913 train_acc= 0.68559 test_loss= 0.69415 test_acc= 0.65251 time= 3.03589\n",
      "Epoch: 0035 train_loss= 0.65480 train_acc= 0.69369 test_loss= 0.69268 test_acc= 0.65726 time= 3.08552\n",
      "Epoch: 0036 train_loss= 0.65046 train_acc= 0.70450 test_loss= 0.69135 test_acc= 0.65162 time= 2.96118\n",
      "Epoch: 0037 train_loss= 0.64599 train_acc= 0.70180 test_loss= 0.69035 test_acc= 0.64360 time= 3.01028\n",
      "Epoch: 0038 train_loss= 0.64154 train_acc= 0.71261 test_loss= 0.68967 test_acc= 0.64330 time= 2.95730\n",
      "Epoch: 0039 train_loss= 0.63651 train_acc= 0.71532 test_loss= 0.68896 test_acc= 0.64479 time= 2.97845\n",
      "Epoch: 0040 train_loss= 0.63156 train_acc= 0.71532 test_loss= 0.68831 test_acc= 0.64746 time= 2.96004\n",
      "Epoch: 0041 train_loss= 0.62659 train_acc= 0.72613 test_loss= 0.68751 test_acc= 0.64924 time= 2.99546\n",
      "Epoch: 0042 train_loss= 0.62196 train_acc= 0.72883 test_loss= 0.68649 test_acc= 0.65548 time= 2.98939\n",
      "Epoch: 0043 train_loss= 0.61764 train_acc= 0.72883 test_loss= 0.68559 test_acc= 0.65786 time= 2.98279\n",
      "Epoch: 0044 train_loss= 0.61346 train_acc= 0.72883 test_loss= 0.68485 test_acc= 0.65251 time= 2.98882\n",
      "Epoch: 0045 train_loss= 0.60940 train_acc= 0.73153 test_loss= 0.68427 test_acc= 0.64657 time= 3.01979\n",
      "Epoch: 0046 train_loss= 0.60548 train_acc= 0.73423 test_loss= 0.68384 test_acc= 0.65281 time= 2.98131\n",
      "Epoch: 0047 train_loss= 0.60157 train_acc= 0.74234 test_loss= 0.68344 test_acc= 0.65578 time= 2.95347\n",
      "Epoch: 0048 train_loss= 0.59775 train_acc= 0.74505 test_loss= 0.68309 test_acc= 0.65845 time= 2.96418\n",
      "Epoch: 0049 train_loss= 0.59401 train_acc= 0.74234 test_loss= 0.68280 test_acc= 0.65934 time= 3.00513\n",
      "Epoch: 0050 train_loss= 0.59035 train_acc= 0.74505 test_loss= 0.68261 test_acc= 0.65934 time= 2.97714\n",
      "Epoch: 0051 train_loss= 0.58668 train_acc= 0.75045 test_loss= 0.68245 test_acc= 0.66290 time= 2.99444\n",
      "Epoch: 0052 train_loss= 0.58304 train_acc= 0.75315 test_loss= 0.68225 test_acc= 0.66825 time= 3.00032\n",
      "Epoch: 0053 train_loss= 0.57954 train_acc= 0.75586 test_loss= 0.68213 test_acc= 0.67152 time= 2.98051\n",
      "Epoch: 0054 train_loss= 0.57605 train_acc= 0.75586 test_loss= 0.68207 test_acc= 0.67389 time= 3.01329\n",
      "Epoch: 0055 train_loss= 0.57258 train_acc= 0.75856 test_loss= 0.68210 test_acc= 0.67746 time= 3.00327\n",
      "Epoch: 0056 train_loss= 0.56913 train_acc= 0.76126 test_loss= 0.68224 test_acc= 0.68162 time= 2.96934\n",
      "Epoch: 0057 train_loss= 0.56571 train_acc= 0.76396 test_loss= 0.68236 test_acc= 0.68429 time= 2.98798\n",
      "Epoch: 0058 train_loss= 0.56236 train_acc= 0.76937 test_loss= 0.68256 test_acc= 0.68934 time= 2.97620\n",
      "Epoch: 0059 train_loss= 0.55899 train_acc= 0.76937 test_loss= 0.68287 test_acc= 0.69231 time= 3.01587\n",
      "Epoch: 0060 train_loss= 0.55568 train_acc= 0.77748 test_loss= 0.68319 test_acc= 0.69231 time= 2.98078\n",
      "early_stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.68218 accuracy= 0.69231 time= 0.060266\n",
      "accuracy, sensivity, specificity, fscore, auc: 0.6923076923076923 0.5853658536585366 0.78 0.6315789473684211 0.691219512195122\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "  tf.reset_default_graph()\n",
    "\n",
    "  # Define placeholders\n",
    "  placeholders = {\n",
    "      'adj': tf.placeholder(tf.float32, shape=(None,FLAGS.node_num,FLAGS.node_num)),\n",
    "      'features': tf.placeholder(tf.float32, shape=(None,FLAGS.node_num,FLAGS.node_num)),\n",
    "      'labels': tf.placeholder(tf.float32, shape=(None, 1)),\n",
    "      'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "      'in_drop': tf.placeholder_with_default(0., shape=()),\n",
    "  }\n",
    "\n",
    "\n",
    "  print(\"\\n=== Fold %d ===\" % k)\n",
    "  train_index = all_train_index[k]\n",
    "  test_index = all_test_index[k]\n",
    "  bestModelSavePath0 = '/home/celery/Documents/Research/AI_Tsinghua_Au_2025/Code/fold_e_mask%s/gat_e%s_weights_best.ckpt' % (str(k),str(k))\n",
    "  bestModelSavePath1 = '/home/celery/Documents/Research/AI_Tsinghua_Au_2025/Code/fold_e_mask%s/gat_e%s_weights_best.ckpt' % (str(k),str(k))\n",
    "  features_train, features_test = features[train_index], features[test_index]\n",
    "  support_train, support_test = adjs[train_index], adjs[test_index]\n",
    "  y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "  support_train, features_train, y_train = shuffle(support_train, features_train, y_train)\n",
    "  support_test, features_test, y_test = shuffle(support_test, features_test, y_test)\n",
    "  print(\"test shape:\",features_test.shape, \"support shape:\", support_train.shape)\n",
    "  print(\"y_test\", y_test[:3])\n",
    "\n",
    "  model = Model(placeholders, input_dim=features.shape[2])\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  '''设置模型保存器'''\n",
    "  m_saver = tf.train.Saver()\n",
    "\n",
    "  cost_val = []\n",
    "  batch_num = FLAGS.batch_num\n",
    "  train_num = features_train.shape[0]\n",
    "  for epoch in range(FLAGS.epochs):  #FLAGS.epochs\n",
    "      batches = range(round(train_num / batch_num))\n",
    "\n",
    "      costs = np.zeros((len(batches), 2))\n",
    "      accs = np.zeros((len(batches), 2))\n",
    "\n",
    "      t = time.time()\n",
    "\n",
    "      for ib in batches:\n",
    "          from_i = ib * batch_num\n",
    "          to_i = (ib+1) * batch_num\n",
    "\n",
    "          features_train_batch = features_train[from_i:to_i]\n",
    "          support_train_batch = support_train[from_i:to_i]\n",
    "          y_train_batch = y_train[from_i:to_i]\n",
    "          feed_dict = construct_feed_dict(features_train_batch, support_train_batch, y_train_batch, placeholders)\n",
    "          feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "          feed_dict.update({placeholders['in_drop']: FLAGS.in_drop})\n",
    "\n",
    "          outs = sess.run([model.opt_op, model.loss, model.accuracy],feed_dict=feed_dict)\n",
    "\n",
    "          cost,acc,duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "          costs[ib] = [outs[1], cost]\n",
    "          accs[ib] = [outs[2], acc]\n",
    "\n",
    "      costs = costs.mean(axis=0)\n",
    "      cost_train, cost_test = costs\n",
    "      cost_val.append(cost_test)\n",
    "\n",
    "      accs = accs.mean(axis=0)\n",
    "      acc_train, acc_test = accs\n",
    "\n",
    "\n",
    "      print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cost_train),\n",
    "          \"train_acc=\", \"{:.5f}\".format(acc_train), \"test_loss=\", \"{:.5f}\".format(cost_test),\n",
    "          \"test_acc=\", \"{:.5f}\".format(acc_test), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "      if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "          print(\"early_stopping...\")\n",
    "          break\n",
    "\n",
    "  m_saver.save(sess, bestModelSavePath0)\n",
    "  print(\"Optimization Finished!\")\n",
    "\n",
    "  #evaluation\n",
    "  test_cost, test_acc, test_duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "  print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "    \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.6f}\".format(test_duration))\n",
    "  l1.append(test_acc)\n",
    "  test_pre = model.predict()\n",
    "  feed_dict = construct_feed_dict(features_test, support_test, y_test, placeholders)\n",
    "  test_pre = sess.run([test_pre],feed_dict=feed_dict)\n",
    "\n",
    "  y_pred = []\n",
    "  for p in test_pre[0]:\n",
    "      y_pred.append(round(p[0]))\n",
    "  y_pred = np.array(y_pred)\n",
    "\n",
    "  [[TN, FP], [FN, TP]] = confusion_matrix(y_test, y_pred, labels=[0, 1]).astype(float)\n",
    "  acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "  specificity = TN/(FP+TN)\n",
    "  sensitivity = recall = TP/(TP+FN)\n",
    "  fscore = f1_score(y_test, y_pred)\n",
    "  fpr, tpr, thresholds = roc_curve(y_test,test_pre[0])\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  print(\"accuracy, sensivity, specificity, fscore, auc:\", acc, sensitivity, specificity, fscore, roc_auc)\n",
    "  result = [acc, sensitivity, specificity, fscore, roc_auc]\n",
    "  full_results.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7321f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6456 ± 0.0343\n",
      "Sensitivity: 0.5463 ± 0.0717\n",
      "Specificity: 0.7265 ± 0.0380\n",
      "F1-score: 0.5785 ± 0.0542\n",
      "AUC: 0.6671 ± 0.0228\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy array for easy column-wise operations\n",
    "full_results_array = np.array(full_results)\n",
    "\n",
    "# Compute average and std deviation for each column\n",
    "means = np.mean(full_results_array, axis=0)\n",
    "stds = np.std(full_results_array, axis=0)\n",
    "\n",
    "# Print nicely (since no f-strings in Python 3.5)\n",
    "metric_names = [\"Accuracy\", \"Sensitivity\", \"Specificity\", \"F1-score\", \"AUC\"]\n",
    "for i in range(len(metric_names)):\n",
    "    print(\"%s: %.4f ± %.4f\" % (metric_names[i], means[i], stds[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
